{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repurchase Within Horizon (Classification) \n",
    "\n",
    "**Goal**  \n",
    "Predict if a user will purchase again within **T ∈ {7, 14, 30, 45}** days.\n",
    "**Data / Anchors**  \n",
    "- Window (UTC): 2024-12-24 → 2025-08-04  \n",
    "- Weekly, calendar-anchored snapshots per user  \n",
    "- Right-censoring: require ≥T days of future data  \n",
    "- At-risk gates: {7:0d, 14:0d, 30:7d, 45:14d}\n",
    "\n",
    "**Label**  \n",
    "`y(T)=1` if the first purchase after anchor occurs ≤T days; else 0.\n",
    "\n",
    "**Features (compact)**  \n",
    "Recency + rolling counts (7/14/30/45d) for key events, last_service, seasonality (month/week + sin/cos), user cadence (inter-purchase mean/median/std).\n",
    "\n",
    "**Model**  \n",
    "Per-horizon **LightGBM (binary)** with early stopping; **Platt calibration** on validation.\n",
    "\n",
    "**Operating thresholds**  \n",
    "- Default: global F1-optimal per T  \n",
    "- Optional: **precision-target** (e.g., ≥0.90) or **quota-based** (flag top-q%)  \n",
    "- JSON thresholds shipped; manual overrides supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/tree/Projects/tubitak-ai-agent/tubitakaiagentprojeleriiinverisetleri\n",
      "OUT_DIR : /Users/tree/Projects/tubitak-ai-agent/notebooks/outputs_b_section\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, json, os, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('/Users/tree/Projects/tubitak-ai-agent/tubitakaiagentprojeleriiinverisetleri')\n",
    "OUT_DIR = Path('outputs_b_section')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_DIR:', DATA_DIR.resolve())\n",
    "print('OUT_DIR :', OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, f1_score,\n",
    "    precision_recall_curve, brier_score_loss, confusion_matrix\n",
    ")\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Could not infer format, so each element will be parsed individually\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "UID_CANDIDATES = [\n",
    "    \"user_id\",\"userid\",\"user\",\"uid\",\"customer_id\",\"client_id\",\n",
    "    \"ownerid\",\"owner_id\",\"userId\",\"ownerId\",\"customerId\",\"clientId\",\"id\"\n",
    "]\n",
    "SERV_CANDIDATES = [\n",
    "    \"serviceType\",\"servicetype\",\"service_type\",\"service\",\"type\",\n",
    "    \"servicename\",\"service_name\",\"category\",\"segment\"\n",
    "]\n",
    "\n",
    "def _lower_map(cols):\n",
    "    return {c.lower(): c for c in cols}\n",
    "\n",
    "def _lookup(cols, candidates):\n",
    "    cmap = _lower_map(cols)\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cmap:\n",
    "            return cmap[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _guess_time_col(df):\n",
    "    cols = list(df.columns)\n",
    "    exact = [\"purchase_time\",\"event_time\",\"signup_time\",\"created_at\",\"createdAt\",\"timestamp\",\"time\",\"date\"]\n",
    "    for e in exact:\n",
    "        for c in cols:\n",
    "            if c.lower() == e.lower():\n",
    "                return c\n",
    "    pri = [\"purchase\",\"event\",\"signup\",\"created\",\"timestamp\",\"time\",\"date\"]\n",
    "    timey = [c for c in cols if any(p in c.lower() for p in pri)]\n",
    "    if timey:\n",
    "        prefer = [c for c in timey if any(c.lower().endswith(suf) for suf in [\"_time\",\"time\",\"timestamp\",\"_at\"])]\n",
    "        return prefer[0] if prefer else timey[0]\n",
    "    return None\n",
    "\n",
    "def normalize_purchase_df(df):\n",
    "    df = df.copy()\n",
    "    uid = _lookup(df.columns, UID_CANDIDATES)\n",
    "    if uid and uid != \"user_id\":\n",
    "        df = df.rename(columns={uid: \"user_id\"})\n",
    "    tcol = _guess_time_col(df)\n",
    "    if tcol and tcol != \"purchase_time\":\n",
    "        df = df.rename(columns={tcol: \"purchase_time\"})\n",
    "    scol = _lookup(df.columns, SERV_CANDIDATES)\n",
    "    if scol and scol != \"serviceType\":\n",
    "        df = df.rename(columns={scol: \"serviceType\"})\n",
    "    if \"user_id\" not in df.columns:\n",
    "        raise KeyError(f\"Could not detect user_id in Purchase.csv: {list(df.columns)}\")\n",
    "    if \"purchase_time\" not in df.columns:\n",
    "        raise KeyError(f\"Could not detect purchase_time in Purchase.csv: {list(df.columns)}\")\n",
    "    if \"serviceType\" not in df.columns:\n",
    "        df[\"serviceType\"] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "def normalize_event_df(df):\n",
    "    df = df.copy()\n",
    "    uid = _lookup(df.columns, UID_CANDIDATES)\n",
    "    if uid and uid != \"user_id\":\n",
    "        df = df.rename(columns={uid: \"user_id\"})\n",
    "    tcol = _guess_time_col(df)\n",
    "    if tcol and tcol != \"event_time\":\n",
    "        df = df.rename(columns={tcol: \"event_time\"})\n",
    "    scol = _lookup(df.columns, SERV_CANDIDATES)\n",
    "    if scol and scol != \"serviceType\":\n",
    "        df = df.rename(columns={scol: \"serviceType\"})\n",
    "    if \"user_id\" not in df.columns:\n",
    "        raise KeyError(f\"Could not detect user_id in event df: {list(df.columns)}\")\n",
    "    if \"event_time\" not in df.columns:\n",
    "        raise KeyError(f\"Could not detect event_time in event df: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def normalize_signup_df(df):\n",
    "    df = df.copy()\n",
    "    uid = _lookup(df.columns, UID_CANDIDATES)\n",
    "    if uid and uid != \"user_id\":\n",
    "        df = df.rename(columns={uid: \"user_id\"})\n",
    "    tcol = _lookup(df.columns, [\"signup_time\",\"signuptime\",\"signed_up_at\",\"created_at\",\"created\",\"time\",\"timestamp\",\"date\"])\n",
    "    if tcol and tcol != \"signup_time\":\n",
    "        df = df.rename(columns={tcol: \"signup_time\"})\n",
    "    if \"signup_time\" not in df.columns:\n",
    "        df[\"signup_time\"] = pd.NaT\n",
    "    if \"user_id\" not in df.columns:\n",
    "        raise KeyError(f\"Could not detect user_id in SignupCompleted.csv: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def to_utc(s):\n",
    "    return pd.to_datetime(s, errors=\"coerce\", utc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchases: (28701, 4) | users: 1614\n",
      "Time span: 2024-12-24 21:41:02.022000+00:00 → 2025-08-04 09:34:52.801000+00:00\n"
     ]
    }
   ],
   "source": [
    "PUR_raw = pd.read_csv(DATA_DIR/'Purchase.csv')\n",
    "SIGN_raw= pd.read_csv(DATA_DIR/'SignupCompleted.csv')\n",
    "BP_raw  = pd.read_csv(DATA_DIR/'BeforePurchaseDetailsScreen.csv')\n",
    "CHK_raw = pd.read_csv(DATA_DIR/'CheckoutPageOpened.csv')\n",
    "ADDR_raw= pd.read_csv(DATA_DIR/'AddressAdded.csv')\n",
    "CARD_raw= pd.read_csv(DATA_DIR/'CreditcardAdded.csv')\n",
    "DOG_raw = pd.read_csv(DATA_DIR/'DogAdded.csv')\n",
    "CAT_raw = pd.read_csv(DATA_DIR/'CatAdded.csv')\n",
    "\n",
    "PUR  = normalize_purchase_df(PUR_raw)\n",
    "SIGN = normalize_signup_df(SIGN_raw)\n",
    "BP   = normalize_event_df(BP_raw)\n",
    "CHK  = normalize_event_df(CHK_raw)\n",
    "ADDR = normalize_event_df(ADDR_raw)\n",
    "CARD = normalize_event_df(CARD_raw)\n",
    "DOG  = normalize_event_df(DOG_raw)\n",
    "CAT  = normalize_event_df(CAT_raw)\n",
    "\n",
    "PUR['purchase_time'] = to_utc(PUR['purchase_time'])\n",
    "SIGN['signup_time']  = to_utc(SIGN['signup_time'])\n",
    "for df in (BP, CHK, ADDR, CARD, DOG, CAT):\n",
    "    df['event_time'] = to_utc(df['event_time'])\n",
    "\n",
    "def keep_minimal(df):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    cols = [c for c in ['user_id','event_time','serviceType'] if c in df.columns]\n",
    "    return df[cols].copy()\n",
    "\n",
    "BP   = keep_minimal(BP)\n",
    "CHK  = keep_minimal(CHK)\n",
    "ADDR = keep_minimal(ADDR)\n",
    "CARD = keep_minimal(CARD)\n",
    "DOG  = keep_minimal(DOG)\n",
    "CAT  = keep_minimal(CAT)\n",
    "\n",
    "PUR = PUR.dropna(subset=['user_id','purchase_time']).sort_values(['user_id','purchase_time'])\n",
    "print('Purchases:', PUR.shape, '| users:', PUR['user_id'].nunique())\n",
    "print('Time span:', PUR['purchase_time'].min(), '→', PUR['purchase_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity span: 2024-12-24 21:41:02.022000+00:00 → 2025-08-04 11:59:43.172000+00:00\n",
      "Weekly anchors (eligibility met): 10616\n"
     ]
    }
   ],
   "source": [
    "def floor_to_week(ts):\n",
    "    return (ts - pd.to_timedelta(ts.dt.weekday, unit='d')).dt.floor('D')\n",
    "\n",
    "all_events = [\n",
    "    PUR[['user_id']].assign(time=PUR['purchase_time']),\n",
    "    BP[['user_id']].assign(time=BP['event_time']),\n",
    "    CHK[['user_id']].assign(time=CHK['event_time']),\n",
    "    ADDR[['user_id']].assign(time=ADDR['event_time']),\n",
    "    CARD[['user_id']].assign(time=CARD['event_time']),\n",
    "    DOG[['user_id']].assign(time=DOG['event_time']),\n",
    "    CAT[['user_id']].assign(time=CAT['event_time']),\n",
    "]\n",
    "AE = pd.concat(all_events, ignore_index=True).dropna(subset=['user_id','time']).sort_values(['user_id','time'])\n",
    "\n",
    "first_seen = AE.groupby('user_id')['time'].min()\n",
    "last_seen  = AE.groupby('user_id')['time'].max()\n",
    "\n",
    "print('Activity span:', AE['time'].min(), '→', AE['time'].max())\n",
    "\n",
    "def user_weeks(u):\n",
    "    fs = first_seen.loc[u]; le = last_seen.loc[u]\n",
    "    start = floor_to_week(pd.Series([fs])).iloc[0]\n",
    "    end   = floor_to_week(pd.Series([le])).iloc[0]\n",
    "    weeks = pd.date_range(start=start, end=end, freq='W-MON', tz='UTC')\n",
    "    return pd.DataFrame({'user_id': u, 't': weeks})\n",
    "\n",
    "anchors = pd.concat([user_weeks(u) for u in first_seen.index], ignore_index=True).sort_values(['user_id','t'])\n",
    "\n",
    "AE_user = AE.set_index('user_id')\n",
    "def had_prior(u, t):\n",
    "    if u not in AE_user.index: return False\n",
    "    g = AE_user.loc[[u]]\n",
    "    if isinstance(g, pd.Series):\n",
    "        return bool(g['time'] < t)\n",
    "    return bool((g['time'] < t).any())\n",
    "\n",
    "mask_prior = anchors.apply(lambda r: had_prior(r['user_id'], r['t']), axis=1)\n",
    "anchors = anchors[mask_prior].copy()\n",
    "print('Weekly anchors (eligibility met):', len(anchors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built features via numpy-asof: rows=10616, prior-purchase coverage=100.0%\n",
      "Feature table shape: (10616, 36)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def ensure_utc(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(s, errors='coerce')\n",
    "    if getattr(s.dt, 'tz', None) is None:\n",
    "        return s.dt.tz_localize('UTC')\n",
    "    else:\n",
    "        return s.dt.tz_convert('UTC')\n",
    "\n",
    "def to_ns(ts: pd.Series) -> np.ndarray:\n",
    "    return ts.astype('int64').to_numpy()\n",
    "\n",
    "anchors['t'] = ensure_utc(anchors['t'])\n",
    "PUR['purchase_time'] = ensure_utc(PUR['purchase_time'])\n",
    "\n",
    "purch = (\n",
    "    PUR[['user_id','purchase_time','serviceType']]\n",
    "      .dropna(subset=['purchase_time'])\n",
    "      .rename(columns={'serviceType':'last_service'})\n",
    "      .sort_values(['user_id','purchase_time'])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "pt_by_user  = {u: to_ns(g['purchase_time']) for u, g in purch.groupby('user_id', sort=True)}\n",
    "svc_by_user = {u: g['last_service'].to_numpy() for u, g in purch.groupby('user_id', sort=True)}\n",
    "\n",
    "anch = anchors[['user_id','t']].copy().sort_values(['user_id','t']).reset_index(drop=True)\n",
    "t_ns = to_ns(anch['t'])\n",
    "\n",
    "pt_ns_out = np.empty(len(anch), dtype=np.int64)\n",
    "svc_out   = np.empty(len(anch), dtype=object)\n",
    "\n",
    "for u, g in anch.groupby('user_id', sort=False):\n",
    "    idxs = g.index.to_numpy()\n",
    "    pt   = pt_by_user.get(u, None)\n",
    "    if pt is None or pt.size == 0:\n",
    "        pt_ns_out[idxs] = -1\n",
    "        svc_out[idxs]   = ''\n",
    "        continue\n",
    "    j = np.searchsorted(pt, t_ns[idxs], side='right') - 1\n",
    "    ok = j >= 0\n",
    "    svc = np.empty(len(idxs), dtype=object); svc[:] = ''\n",
    "    svc[ok] = svc_by_user[u][j[ok]]\n",
    "    out_pt = np.full(len(idxs), -1, dtype=np.int64); out_pt[ok] = pt[j[ok]]\n",
    "    pt_ns_out[idxs] = out_pt\n",
    "    svc_out[idxs]   = svc\n",
    "\n",
    "fe = anch.copy()\n",
    "fe['pt'] = pd.to_datetime(pt_ns_out, utc=True)\n",
    "fe['last_service'] = svc_out.astype(str)\n",
    "fe['recency_days'] = (fe['t'].astype('int64') - fe['pt'].astype('int64')) / 1e9 / 86400.0\n",
    "\n",
    "print(\"Built features via numpy-asof: rows=%d, prior-purchase coverage=%.1f%%\" % (\n",
    "    len(fe), 100.0 * (~fe['pt'].isna()).mean()\n",
    "))\n",
    "\n",
    "# Rolling windows\n",
    "def counts_in_windows(events_df, time_col, windows, anchors_df, anchor_time_col='t', key='user_id'):\n",
    "    \"\"\"Return a dict: {w: series_of_counts} aligned with anchors_df index, counting events in (t-w, t].\"\"\"\n",
    "    out = {}\n",
    "    # Pre-sort for efficiency\n",
    "    events_df = events_df.dropna(subset=[key, time_col]).copy()\n",
    "    events_df[time_col] = pd.to_datetime(events_df[time_col], errors='coerce', utc=True)\n",
    "    events_df = events_df.dropna(subset=[time_col]).sort_values([key, time_col])\n",
    "    # Build index per user for quick slicing\n",
    "    grouped ={u: g for u, g in events_df.groupby(key)}\n",
    "\n",
    "    users = anchors_df[key]\n",
    "    times = pd.to_datetime(anchors_df[anchor_time_col], errors='coerce', utc=True)\n",
    "    \n",
    "    out = {}\n",
    "    for w in windows:\n",
    "        vals = np.zeros(len(anchors_df), dtype=np.int32)\n",
    "        delta = pd.Timedelta(days=int(w))\n",
    "        for i in range(len(anchors_df)):\n",
    "            u = users.iloc[i]\n",
    "            t = times.iloc[i]\n",
    "            g = grouped.get(u)\n",
    "            if g is None:\n",
    "                continue\n",
    "            t0 = t - delta\n",
    "            mask = (g[time_col] > t0) & (g[time_col] <= t)\n",
    "            vals[i] = mask.sum()\n",
    "        out[w] = pd.Series(vals, index=anchors_df.index)\n",
    "    return out\n",
    "\n",
    "WINS = [7,14,30,60,90]\n",
    "p_counts   = counts_in_windows(PUR.rename(columns={'purchase_time':'time'}), 'time', WINS, fe)\n",
    "bp_counts  = counts_in_windows(BP,   'event_time', WINS, fe)\n",
    "chk_counts = counts_in_windows(CHK,  'event_time', WINS, fe)\n",
    "addr_counts= counts_in_windows(ADDR, 'event_time', WINS, fe)\n",
    "card_counts= counts_in_windows(CARD, 'event_time', WINS, fe)\n",
    "\n",
    "for w in WINS:\n",
    "    fe[f'pur_{w}d']  = p_counts[w]\n",
    "    fe[f'bp_{w}d']   = bp_counts[w]\n",
    "    fe[f'chk_{w}d']  = chk_counts[w]\n",
    "    fe[f'addr_{w}d'] = addr_counts[w]\n",
    "    fe[f'card_{w}d'] = card_counts[w]\n",
    "\n",
    "for a,b in [(7,90),(30,90)]:\n",
    "    fe[f'pur_ratio_{a}on{b}'] = (fe[f'pur_{a}d']+1e-3)/(fe[f'pur_{b}d']+1e-3)\n",
    "\n",
    "SIGN = SIGN.drop_duplicates(subset=['user_id'])\n",
    "fe = fe.merge(SIGN[['user_id','signup_time']], on='user_id', how='left')\n",
    "fe['days_since_signup'] = (fe['t'] - fe['signup_time']).dt.total_seconds()/86400.0\n",
    "fe['dow'] = fe['t'].dt.weekday\n",
    "fe['hour'] = fe['t'].dt.hour\n",
    "\n",
    "print('Feature table shape:', fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0daaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=7: n=10,577 | positives=4,359 (41.212%) | numeric_feats=31 | has last_service=True\n",
      "T=14: n=10,065 | positives=5,541 (55.052%) | numeric_feats=31 | has last_service=True\n",
      "T=30: n=7,609 | positives=5,361 (70.456%) | numeric_feats=31 | has last_service=True\n",
      "T=45: n=5,702 | positives=4,405 (77.254%) | numeric_feats=31 | has last_service=True\n"
     ]
    }
   ],
   "source": [
    "# rebuild labels while PRESERVING features from `fe`\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HORIZONS = [7, 14, 30, 45]\n",
    "\n",
    "# Safety: ensure tz-aware\n",
    "fe['t'] = pd.to_datetime(fe['t'], errors='coerce', utc=True)\n",
    "PUR['purchase_time'] = pd.to_datetime(PUR['purchase_time'], errors='coerce', utc=True)\n",
    "\n",
    "# Precompute per-user sorted purchase times (epoch ns)\n",
    "purch_sorted = (PUR.dropna(subset=['user_id','purchase_time'])\n",
    "                  .sort_values(['user_id','purchase_time']))\n",
    "purch_sorted_ns = purch_sorted.assign(p_ns=purch_sorted['purchase_time'].astype('int64'))\n",
    "p_ns_by_user = {u: g['p_ns'].to_numpy() for u, g in purch_sorted_ns.groupby('user_id', sort=True)}\n",
    "max_purchase_ns = purch_sorted_ns['p_ns'].max()\n",
    "\n",
    "# Keep ALL feature columns from fe\n",
    "fe_sorted = fe.sort_values(['user_id','t']).reset_index(drop=True)\n",
    "t_ns_all  = fe_sorted['t'].astype('int64').to_numpy()\n",
    "\n",
    "labels = {}  # rebuilt, with features intact\n",
    "\n",
    "for T in HORIZONS:\n",
    "    T_ns = int(T * 86400 * 1e9)\n",
    "\n",
    "    # Right-censor: only anchors where we could observe a purchase within T\n",
    "    keep = t_ns_all <= (max_purchase_ns - T_ns)\n",
    "    sub  = fe_sorted.loc[keep].copy()   # <-- ALL columns preserved\n",
    "\n",
    "    # Vectorized next-purchase-within-T labeling\n",
    "    y = np.zeros(len(sub), dtype=np.int8)\n",
    "    # group indices by user to avoid row loops\n",
    "    idxs_by_user = sub.groupby('user_id').indices\n",
    "    t_ns = sub['t'].astype('int64').to_numpy()\n",
    "    for u, idxs in idxs_by_user.items():\n",
    "        p_ns = p_ns_by_user.get(u)\n",
    "        if p_ns is None or p_ns.size == 0:\n",
    "            continue\n",
    "        j = np.searchsorted(p_ns, t_ns[idxs], side='right')  # first purchase strictly after t\n",
    "        valid = j < p_ns.size\n",
    "        y_u = np.zeros(len(idxs), dtype=np.int8)\n",
    "        y_u[valid] = ((p_ns[j[valid]] - t_ns[idxs][valid]) <= T_ns).astype(np.int8)\n",
    "        y[idxs] = y_u\n",
    "\n",
    "    sub['y'] = y\n",
    "    labels[T] = sub\n",
    "\n",
    "    # Diagnostics: confirm features exist\n",
    "    pref = ['pur_','bp_','chk_','addr_','card_','pur_ratio_','recency','days_since_signup','dow','hour']\n",
    "    num_cols = [c for c in sub.columns if any(c.startswith(p) for p in pref)]\n",
    "    print(f\"T={T}: n={len(sub):,} | positives={int(y.sum()):,} ({y.mean():.3%}) \"\n",
    "          f\"| numeric_feats={len(num_cols)} | has last_service={'last_service' in sub.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb3bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FE_PREFIXES = [\n",
    "    'pur_', 'bp_', 'chk_', 'addr_', 'card_', 'pur_ratio_', 'recency', 'days_since_signup', 'dow', 'hour'\n",
    "]\n",
    "NON_FEATURE_COLS = {'user_id','t','pt','signup_time','y'} \n",
    "\n",
    "def detect_feature_cols(df: pd.DataFrame):\n",
    "    \"\"\"Return (num_cols, cat_cols) robustly, with fallbacks if prefix scan is empty.\"\"\"\n",
    "    num_cols = [c for c in df.columns\n",
    "                if any(c.startswith(p) for p in FE_PREFIXES)\n",
    "                and c not in NON_FEATURE_COLS]\n",
    "    if len(num_cols) == 0:\n",
    "        num_cols = [c for c in df.columns\n",
    "                    if c not in NON_FEATURE_COLS\n",
    "                    and pd.api.types.is_numeric_dtype(df[c])\n",
    "                    and not pd.api.types.is_categorical_dtype(df[c])]\n",
    "    cat_cols = ['last_service'] if 'last_service' in df.columns else []\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def month_split(df_sub, time_col='t'):\n",
    "    mkey = df_sub[time_col].dt.tz_convert('UTC').dt.strftime('%Y-%m')\n",
    "    months = sorted(mkey.unique())\n",
    "    if len(months) < 3:\n",
    "        tmin, tmax = df_sub[time_col].min(), df_sub[time_col].max()\n",
    "        cut1 = tmin + (tmax - tmin)*0.6\n",
    "        cut2 = tmin + (tmax - tmin)*0.8\n",
    "        tr = df_sub[df_sub[time_col] < cut1]\n",
    "        va = df_sub[(df_sub[time_col] >= cut1) & (df_sub[time_col] < cut2)]\n",
    "        te = df_sub[df_sub[time_col] >= cut2]\n",
    "        return tr, va, te, ('percentile', cut1, cut2)\n",
    "    test_m = months[-1]; val_m = months[-2]; train_m = months[:-2]\n",
    "    tr = df_sub[mkey.isin(train_m)]\n",
    "    va = df_sub[mkey == val_m]\n",
    "    te = df_sub[mkey == test_m]\n",
    "    return tr, va, te, ('months', train_m, val_m, test_m)\n",
    "\n",
    "def prepare_xy(df_sub: pd.DataFrame, enc: OrdinalEncoder|None=None):\n",
    "    num_cols, cat_cols = detect_feature_cols(df_sub)\n",
    "    print(f\"[prepare_xy] num={len(num_cols)} cat={len(cat_cols)} \"\n",
    "          f\"examples={len(df_sub)} | first few num: {num_cols[:5]}\")\n",
    "\n",
    "    X_num = df_sub[num_cols].astype(float).fillna(0.0).values if num_cols else np.empty((len(df_sub),0))\n",
    "    if cat_cols:\n",
    "        if enc is None:\n",
    "            enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            X_cat = enc.fit_transform(df_sub[cat_cols].astype(str))\n",
    "        else:\n",
    "            X_cat = enc.transform(df_sub[cat_cols].astype(str))\n",
    "        X = np.hstack([X_num, X_cat]) if X_num.size else X_cat\n",
    "    else:\n",
    "        X = X_num\n",
    "    y = df_sub['y'].astype(int).values\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\n",
    "            \"No features detected. Check your feature columns. \"\n",
    "            f\"Available columns: {list(df_sub.columns)}\"\n",
    "        )\n",
    "    return X, y, enc, (num_cols + cat_cols)\n",
    "\n",
    "def pick_f1_threshold(y, p):\n",
    "    if np.unique(y).size < 2:\n",
    "        return float(np.clip(y.mean(), 0.01, 0.99))\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    f1s = 2*prec*rec/(prec+rec+1e-9)\n",
    "    return float(thr[np.nanargmax(f1s)]) if len(thr) else float(y.mean())\n",
    "\n",
    "def ece_score(y_true, y_prob, bins=10):\n",
    "    edges = np.linspace(0,1,bins+1)\n",
    "    idx = np.digitize(y_prob, edges) - 1\n",
    "    N = len(y_true); ece = 0.0\n",
    "    for b in range(bins):\n",
    "        m = (idx==b)\n",
    "        if m.sum()==0: continue\n",
    "        ece += (m.sum()/N) * abs(float(y_prob[m].mean()) - float(y_true[m].mean()))\n",
    "    return float(ece)\n",
    "\n",
    "def calibrator_from_dev(y_dev, p_dev):\n",
    "    if (len(y_dev) >= 200) and (np.unique(y_dev).size==2):\n",
    "        try:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso.fit(p_dev, y_dev)\n",
    "            return lambda p: iso.transform(p), 'isotonic'\n",
    "        except Exception:\n",
    "            pass\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(p_dev.reshape(-1,1), y_dev)\n",
    "    return lambda p: lr.predict_proba(p.reshape(-1,1))[:,1], 'platt'\n",
    "\n",
    "def safe_roc(y,p): \n",
    "    return float(roc_auc_score(y,p)) if np.unique(y).size==2 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601d70da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== T=7 split: ('months', ['2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05'], '2025-06', '2025-07') ===\n",
      "[prepare_xy] num=31 cat=1 examples=2792 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=4817 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=2968 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "X_tr shape: (2792, 32) | X_va: (4817, 32) | X_te: (2968, 32)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.775759\tvalid's average_precision: 0.699733\n",
      "T=7  thr=0.326  PR-AUC(val)=0.692  PR-AUC(test)=0.755  F1(test)=0.718\n",
      "Confusion (test) [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[1027  568]\n",
      " [ 287 1086]]\n",
      "\n",
      "=== T=14 split: ('months', ['2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05'], '2025-06', '2025-07') ===\n",
      "[prepare_xy] num=31 cat=1 examples=2792 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=4817 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=2456 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "X_tr shape: (2792, 32) | X_va: (4817, 32) | X_te: (2456, 32)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's auc: 0.789038\tvalid's average_precision: 0.808218\n",
      "T=14  thr=0.429  PR-AUC(val)=0.800  PR-AUC(test)=0.855  F1(test)=0.809\n",
      "Confusion (test) [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[ 672  335]\n",
      " [ 238 1211]]\n",
      "\n",
      "=== T=30 split: ('months', ['2024-12', '2025-01', '2025-02', '2025-03', '2025-04'], '2025-05', '2025-06') ===\n",
      "[prepare_xy] num=31 cat=1 examples=426 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=2366 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=4817 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "X_tr shape: (426, 32) | X_va: (2366, 32) | X_te: (4817, 32)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.681384\tvalid's average_precision: 0.853067\n",
      "T=30  thr=0.602  PR-AUC(val)=0.862  PR-AUC(test)=0.829  F1(test)=0.818\n",
      "Confusion (test) [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[   0 1484]\n",
      " [   0 3333]]\n",
      "\n",
      "=== T=45 split: ('months', ['2024-12', '2025-01', '2025-02', '2025-03', '2025-04'], '2025-05', '2025-06') ===\n",
      "[prepare_xy] num=31 cat=1 examples=426 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=2366 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "[prepare_xy] num=31 cat=1 examples=2910 | first few num: ['recency_days', 'pur_7d', 'bp_7d', 'chk_7d', 'addr_7d']\n",
      "X_tr shape: (426, 32) | X_va: (2366, 32) | X_te: (2910, 32)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid's auc: 0.406105\tvalid's average_precision: 0.80049\n",
      "T=45  thr=0.794  PR-AUC(val)=0.844  PR-AUC(test)=0.776  F1(test)=0.859\n",
      "Confusion (test) [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[   0  718]\n",
      " [   0 2192]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>split</th>\n",
       "      <th>val_pr_auc</th>\n",
       "      <th>val_roc_auc</th>\n",
       "      <th>val_brier</th>\n",
       "      <th>val_ece</th>\n",
       "      <th>thr_global</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_brier</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_rate_train</th>\n",
       "      <th>pos_rate_val</th>\n",
       "      <th>pos_rate_test</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_tp</th>\n",
       "      <th>test_fp</th>\n",
       "      <th>test_fn</th>\n",
       "      <th>test_tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>months</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.7548</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.4626</td>\n",
       "      <td>0.6566</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>1086</td>\n",
       "      <td>568</td>\n",
       "      <td>287</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>months</td>\n",
       "      <td>0.7998</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>0.1831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.8548</td>\n",
       "      <td>0.8292</td>\n",
       "      <td>0.1733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5534</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>1211</td>\n",
       "      <td>335</td>\n",
       "      <td>238</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>months</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.1821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4836</td>\n",
       "      <td>0.7701</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8179</td>\n",
       "      <td>3333</td>\n",
       "      <td>1484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>months</td>\n",
       "      <td>0.8441</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6432</td>\n",
       "      <td>0.8195</td>\n",
       "      <td>0.7533</td>\n",
       "      <td>0.7533</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>2192</td>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    T   split  val_pr_auc  val_roc_auc  val_brier  val_ece  thr_global  \\\n",
       "0   7  months      0.6919       0.7790     0.1782      0.0      0.3261   \n",
       "1  14  months      0.7998       0.7933     0.1831      0.0      0.4286   \n",
       "2  30  months      0.8616       0.7156     0.1543      0.0      0.6025   \n",
       "3  45  months      0.8441       0.5717     0.1449      0.0      0.7936   \n",
       "\n",
       "   test_pr_auc  test_roc_auc  test_brier  ...  pos_rate_train pos_rate_val  \\\n",
       "0       0.7548        0.7977      0.1949  ...          0.4158       0.3789   \n",
       "1       0.8548        0.8292      0.1733  ...          0.5534       0.5288   \n",
       "2       0.8290        0.7422      0.1821  ...          0.4836       0.7701   \n",
       "3       0.7762        0.5547      0.1869  ...          0.6432       0.8195   \n",
       "\n",
       "   pos_rate_test  test_precision  test_recall  test_f1  test_tp  test_fp  \\\n",
       "0         0.4626          0.6566       0.7910   0.7175     1086      568   \n",
       "1         0.5900          0.7833       0.8357   0.8087     1211      335   \n",
       "2         0.6919          0.6919       1.0000   0.8179     3333     1484   \n",
       "3         0.7533          0.7533       1.0000   0.8593     2192      718   \n",
       "\n",
       "   test_fn  test_tn  \n",
       "0      287     1027  \n",
       "1      238      672  \n",
       "2        0        0  \n",
       "3        0        0  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_b_section/repurchase_weekly_propensity_metrics.csv\n",
      "Saved: outputs_b_section/thresholds.json\n"
     ]
    }
   ],
   "source": [
    "HORIZONS = [7,14,30,45]\n",
    "metrics_rows = []\n",
    "thresholds = {'global':{}}\n",
    "\n",
    "for T in HORIZONS:\n",
    "    sub = labels[T].copy()         # labels[T] contains fe columns + 'y'\n",
    "    tr, va, te, split_info = month_split(sub, 't')\n",
    "    print(f\"\\n=== T={T} split:\", split_info, \"===\")\n",
    "    X_tr, y_tr, enc, feats = prepare_xy(tr, None)\n",
    "    X_va, y_va, _, _ = prepare_xy(va, enc)\n",
    "    X_te, y_te, _, _ = prepare_xy(te, enc)\n",
    "\n",
    "    # handle imbalance proportionally to val-set (or train); here from train\n",
    "    pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "    spw = float(neg/pos) if pos>0 else 1.0\n",
    "    print(f\"X_tr shape: {X_tr.shape} | X_va: {X_va.shape} | X_te: {X_te.shape}\")\n",
    "\n",
    "    params = dict(\n",
    "        objective='binary', metric=['auc','average_precision'],\n",
    "        learning_rate=0.05, num_leaves=63, min_data_in_leaf=50,\n",
    "        feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
    "        max_depth=-1, verbose=-1, scale_pos_weight=spw\n",
    "    )\n",
    "\n",
    "    # safety: remove forced params\n",
    "    for k in list(params.keys()):\n",
    "        if 'forced' in k:\n",
    "            params.pop(k, None)\n",
    "\n",
    "    dtr = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n",
    "    dva = lgb.Dataset(X_va, label=y_va, free_raw_data=False)\n",
    "    model = lgb.train(\n",
    "        params, dtr, num_boost_round=2000,\n",
    "        valid_sets=[dva], valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    # calibration on validation\n",
    "    p_va_raw = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    calibrate, cal_name = calibrator_from_dev(y_va, p_va_raw)\n",
    "    p_va = calibrate(p_va_raw)\n",
    "\n",
    "    # global threshold (F1 on val)\n",
    "    thr = pick_f1_threshold(y_va, p_va)\n",
    "    thresholds['global'][str(T)] = float(thr)\n",
    "\n",
    "    # test\n",
    "    p_te_raw = model.predict(X_te, num_iteration=model.best_iteration)\n",
    "    p_te = calibrate(p_te_raw)\n",
    "\n",
    "    # metrics\n",
    "    row = dict(\n",
    "        T=T, split=str(split_info[0]),\n",
    "        val_pr_auc=float(average_precision_score(y_va, p_va)) if y_va.sum()>0 else 0.0,\n",
    "        val_roc_auc=safe_roc(y_va, p_va),\n",
    "        val_brier=float(brier_score_loss(y_va, p_va)),\n",
    "        val_ece=float(ece_score(y_va, p_va)),\n",
    "        thr_global=thr,\n",
    "        test_pr_auc=float(average_precision_score(y_te, p_te)) if y_te.sum()>0 else 0.0,\n",
    "        test_roc_auc=safe_roc(y_te, p_te),\n",
    "        test_brier=float(brier_score_loss(y_te, p_te)),\n",
    "        test_ece=float(ece_score(y_te, p_te)),\n",
    "        cal=cal_name,\n",
    "        n_train=len(y_tr), n_val=len(y_va), n_test=len(y_te),\n",
    "        pos_rate_train=float(y_tr.mean()), pos_rate_val=float(y_va.mean()), pos_rate_test=float(y_te.mean())\n",
    "    )\n",
    "\n",
    "    # confusion matrix at global threshold\n",
    "    y_hat = (p_te >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec+1e-9) if (prec+rec)>0 else 0.0\n",
    "    row.update(dict(test_precision=prec, test_recall=rec, test_f1=f1, test_tp=int(tp),\n",
    "                    test_fp=int(fp), test_fn=int(fn), test_tn=int(tn)))\n",
    "\n",
    "    print(f\"T={T}  thr={thr:.3f}  PR-AUC(val)={row['val_pr_auc']:.3f}  \"\n",
    "          f\"PR-AUC(test)={row['test_pr_auc']:.3f}  F1(test)={row['test_f1']:.3f}\")\n",
    "    print(\"Confusion (test) [rows y_true 0/1, cols y_hat 0/1]:\\n\", cm)\n",
    "\n",
    "    # save per-T predictions\n",
    "    pd.DataFrame({'p_val_raw':p_va_raw, 'p_val':p_va, 'y_val':y_va}).to_csv(OUT_DIR/f'p_hat_val_T{T}.csv', index=False)\n",
    "    pd.DataFrame({'p_test_raw':p_te_raw, 'p_test':p_te, 'y_test':y_te}).to_csv(OUT_DIR/f'p_hat_test_T{T}.csv', index=False)\n",
    "\n",
    "    metrics_rows.append(row)\n",
    "\n",
    "# summary table + thresholds.json\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values('T')\n",
    "metrics_df.to_csv(OUT_DIR/'repurchase_weekly_propensity_metrics.csv', index=False)\n",
    "with open(OUT_DIR/'thresholds.json','w') as f:\n",
    "    json.dump({'global': thresholds['global']}, f, indent=2)\n",
    "\n",
    "display(metrics_df.round(4))\n",
    "print('Saved:', OUT_DIR/'repurchase_weekly_propensity_metrics.csv')\n",
    "print('Saved:', OUT_DIR/'thresholds.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[enrichment] Added columns: ['ip_mean_days', 'ip_std_days', 'ip_median_days', 'ip_min_days', 'ip_max_days', 'ip_count', 'svc_entropy', 'n_services_lifetime']\n",
      "[enrichment] fe shape: (10616, 50)\n",
      "[labels] T=7: n=10,577 | positives=4,359 (41.212%) | at_risk≥0d | feats=48 (excl user_id,t,y)\n",
      "[labels] T=14: n=10,065 | positives=5,541 (55.052%) | at_risk≥0d | feats=48 (excl user_id,t,y)\n",
      "[labels] T=30: n=4,324 | positives=2,439 (56.406%) | at_risk≥7d | feats=48 (excl user_id,t,y)\n",
      "[labels] T=45: n=2,170 | positives=1,221 (56.267%) | at_risk≥14d | feats=48 (excl user_id,t,y)\n"
     ]
    }
   ],
   "source": [
    "# Config, at-risk gate, feature enrichment, rebuild labels ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Horizons & at-risk gates (days of inactivity required at anchor)\n",
    "HORIZONS   = [7, 14, 30, 45]\n",
    "AT_RISK_MIN = {7: 0, 14: 0, 30: 7, 45: 14}   # tweak as needed\n",
    "\n",
    "# Preconditions / safety checks \n",
    "req_fe_cols  = {'user_id','t','recency_days'}\n",
    "req_pur_cols = {'user_id','purchase_time','serviceType'}\n",
    "if not req_fe_cols.issubset(fe.columns):\n",
    "    raise KeyError(f\"`fe` must contain {sorted(req_fe_cols)}; found: {list(fe.columns)}\")\n",
    "if not req_pur_cols.issubset(PUR.columns):\n",
    "    raise KeyError(f\"`PUR` must contain {sorted(req_pur_cols)}; found: {list(PUR.columns)}\")\n",
    "\n",
    "# Normalize dtypes & timezone awareness \n",
    "fe = fe.copy()\n",
    "PUR = PUR.copy()\n",
    "fe['user_id']  = fe['user_id'].astype(str)\n",
    "PUR['user_id'] = PUR['user_id'].astype(str)\n",
    "fe['t'] = pd.to_datetime(fe['t'], utc=True, errors='coerce')\n",
    "PUR['purchase_time'] = pd.to_datetime(PUR['purchase_time'], utc=True, errors='coerce')\n",
    "\n",
    "# de-dup per (user, timestamp, serviceType)\n",
    "PUR = PUR.drop_duplicates(subset=['user_id','purchase_time','serviceType'], keep='first')\n",
    "\n",
    "\n",
    "# 1) Cadence features per user (robust; no arrays)\n",
    "p = (PUR.dropna(subset=['user_id','purchase_time'])\n",
    "        .sort_values(['user_id','purchase_time'], kind='mergesort'))\n",
    "# Interpurchase interval (days) – NaN for each user's first purchase\n",
    "p['ip_days'] = p.groupby('user_id')['purchase_time'].diff().dt.total_seconds() / 86400.0\n",
    "\n",
    "# Aggregate over actual intervals (exclude NaNs)\n",
    "cad = (p.dropna(subset=['ip_days'])\n",
    "         .groupby('user_id', sort=True)['ip_days']\n",
    "         .agg(ip_mean_days='mean',\n",
    "              ip_std_days='std',\n",
    "              ip_median_days='median',\n",
    "              ip_min_days='min',\n",
    "              ip_max_days='max',\n",
    "              ip_count='count')\n",
    "         .reset_index())\n",
    "\n",
    "# Ensure all users are present (fill users with <2 purchases)\n",
    "all_users = pd.DataFrame({'user_id': pd.Index(\n",
    "    sorted(set(fe['user_id']) | set(PUR['user_id']))\n",
    ")})\n",
    "cad = (all_users.merge(cad, on='user_id', how='left')\n",
    "               .fillna({'ip_mean_days': 0.0,\n",
    "                        'ip_std_days': 0.0,\n",
    "                        'ip_median_days': 0.0,\n",
    "                        'ip_min_days': 0.0,\n",
    "                        'ip_max_days': 0.0,\n",
    "                        'ip_count': 0}))\n",
    "\n",
    "# 2) Lifetime service diversity: #services + entropy\n",
    "svc_counts = (PUR.groupby(['user_id','serviceType'])\n",
    "                .size().rename('cnt').reset_index())\n",
    "if not svc_counts.empty:\n",
    "    svc_total  = svc_counts.groupby('user_id')['cnt'].sum().rename('tot')\n",
    "    svc_probs  = svc_counts.merge(svc_total, on='user_id')\n",
    "    svc_probs['p'] = svc_probs['cnt'] / svc_probs['tot'].replace(0, np.nan)\n",
    "    # entropy = - Σ p log p\n",
    "    svc_entropy = (svc_probs.assign(h=lambda r: -(r['p'] * np.log(r['p'] + 1e-12)))\n",
    "                            .groupby('user_id', sort=True)['h'].sum()\n",
    "                            .rename('svc_entropy').reset_index())\n",
    "    svc_nuniq   = (svc_counts.groupby('user_id', sort=True).size()\n",
    "                               .rename('n_services_lifetime').reset_index())\n",
    "else:\n",
    "    svc_entropy = pd.DataFrame({'user_id': all_users['user_id'], 'svc_entropy': 0.0})\n",
    "    svc_nuniq   = pd.DataFrame({'user_id': all_users['user_id'], 'n_services_lifetime': 0})\n",
    "\n",
    "# Combine cadence + diversity\n",
    "cad_all = (cad.merge(svc_entropy, on='user_id', how='left')\n",
    "              .merge(svc_nuniq,   on='user_id', how='left')\n",
    "              .fillna({'svc_entropy': 0.0, 'n_services_lifetime': 0}))\n",
    "\n",
    "# 3) Seasonality features (at anchor time t)\n",
    "fe['month']      = fe['t'].dt.month.astype(int)\n",
    "fe['weekofyear'] = fe['t'].dt.isocalendar().week.astype(int)\n",
    "fe['month_sin']  = np.sin(2*np.pi*(fe['month']/12.0))\n",
    "fe['month_cos']  = np.cos(2*np.pi*(fe['month']/12.0))\n",
    "fe['woy_sin']    = np.sin(2*np.pi*(fe['weekofyear']/52.0))\n",
    "fe['woy_cos']    = np.cos(2*np.pi*(fe['weekofyear']/52.0))\n",
    "\n",
    "# Merge all user-level features into fe\n",
    "before_cols = set(fe.columns)\n",
    "fe = fe.merge(cad_all, on='user_id', how='left')\n",
    "added_cols = [c for c in fe.columns if c not in before_cols]\n",
    "print(f\"[enrichment] Added columns: {added_cols}\")\n",
    "print(f\"[enrichment] fe shape: {fe.shape}\")\n",
    "\n",
    "# 4) Rebuild labels (vectorized) for “purchase within T days”\n",
    "# Right-censor anchors that don't have T days of future data\n",
    "# Apply at-risk gate per T: recency_days ≥ AT_RISK_MIN[T]\n",
    "# Precompute per-user purchase timestamps (ns)\n",
    "p_ns_by_user = {\n",
    "    u: g['purchase_time'].astype('int64').to_numpy()\n",
    "    for u, g in PUR.sort_values(['user_id','purchase_time'], kind='mergesort') \\\n",
    "                   .groupby('user_id', sort=True)\n",
    "}\n",
    "\n",
    "fe_sorted = fe.sort_values(['user_id','t'], kind='mergesort').reset_index(drop=True)\n",
    "t_all_ns  = fe_sorted['t'].astype('int64').to_numpy()\n",
    "max_pt_ns = int(PUR['purchase_time'].astype('int64').max())\n",
    "\n",
    "labels = {}\n",
    "for T in HORIZONS:\n",
    "    T_ns = int(T * 86400 * 1e9)  # days -> ns\n",
    "    # Right-censor: require enough future horizon\n",
    "    keep = t_all_ns <= (max_pt_ns - T_ns)\n",
    "    sub  = fe_sorted.loc[keep].copy()\n",
    "\n",
    "    # At-risk gate by recency\n",
    "    gate = int(AT_RISK_MIN.get(T, 0))\n",
    "    if gate > 0:\n",
    "        if 'recency_days' not in sub.columns:\n",
    "            raise KeyError(\"`recency_days` missing in fe; required for at-risk gate.\")\n",
    "        sub = sub[sub['recency_days'] >= gate].copy()\n",
    "\n",
    "    # Vectorized label construction per user\n",
    "    y = np.zeros(len(sub), dtype=np.int8)\n",
    "    t_ns = sub['t'].astype('int64').to_numpy()\n",
    "    idxs_by_user = sub.groupby('user_id', sort=True).indices # {user: [idxs in sub]}\n",
    "    for u, idxs in idxs_by_user.items():\n",
    "        pts = p_ns_by_user.get(u)\n",
    "        if pts is None or pts.size == 0:\n",
    "            continue\n",
    "        # index of first purchase strictly after anchor time\n",
    "        j = np.searchsorted(pts, t_ns[idxs], side='right')\n",
    "        ok = j < pts.size\n",
    "        # purchase occurs within T days?\n",
    "        y_u = np.zeros(len(idxs), dtype=np.int8)\n",
    "        y_u[ok] = ((pts[j[ok]] - t_ns[idxs][ok]) <= T_ns).astype(np.int8)\n",
    "        y[idxs] = y_u\n",
    "\n",
    "    sub['y'] = y\n",
    "    labels[T] = sub\n",
    "\n",
    "    print(f\"[labels] T={T}: n={len(sub):,} | positives={int(y.sum()):,} \"\n",
    "          f\"({y.mean():.3%}) | at_risk≥{AT_RISK_MIN[T]}d | feats={sub.shape[1]-3} (excl user_id,t,y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69defcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== T=7 | model=lgb | manual thr=0.480 ===\n",
      "Confusion [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[1465  130]\n",
      " [ 809  564]]\n",
      "{'precision': np.float64(0.813), 'recall': np.float64(0.411), 'f1': np.float64(0.546), 'specificity': np.float64(0.918), 'tp': 564, 'fp': 130, 'fn': 809, 'tn': 1465, 'flagged': 694, 'n': 2968, 'flagged_rate': 0.234} | base_rate= 0.463 | AP= 0.755 | ROC= 0.798 | Brier= 0.195\n",
      "Saved flagged: outputs_b_section/flagged_test_T7_manual_thr0.480.lgb.csv\n",
      "\n",
      "=== T=14 | model=lgb | manual thr=0.490 ===\n",
      "Confusion [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[ 713  294]\n",
      " [ 283 1166]]\n",
      "{'precision': np.float64(0.799), 'recall': np.float64(0.805), 'f1': np.float64(0.802), 'specificity': np.float64(0.708), 'tp': 1166, 'fp': 294, 'fn': 283, 'tn': 713, 'flagged': 1460, 'n': 2456, 'flagged_rate': 0.594} | base_rate= 0.59 | AP= 0.855 | ROC= 0.829 | Brier= 0.173\n",
      "Saved flagged: outputs_b_section/flagged_test_T14_manual_thr0.490.lgb.csv\n",
      "[warn] T=30: test slice length 2989 != preds 4817; writing without ids/t.\n",
      "\n",
      "=== T=30 | model=lgb | manual thr=0.630 ===\n",
      "Confusion [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[1108  376]\n",
      " [1002 2331]]\n",
      "{'precision': np.float64(0.861), 'recall': np.float64(0.699), 'f1': np.float64(0.772), 'specificity': np.float64(0.747), 'tp': 2331, 'fp': 376, 'fn': 1002, 'tn': 1108, 'flagged': 2707, 'n': 4817, 'flagged_rate': 0.562} | base_rate= 0.692 | AP= 0.829 | ROC= 0.742 | Brier= 0.182\n",
      "Saved flagged: outputs_b_section/flagged_test_T30_manual_thr0.630.lgb.csv\n",
      "[warn] T=45: test slice length 1262 != preds 2910; writing without ids/t.\n",
      "\n",
      "=== T=45 | model=lgb | manual thr=0.900 ===\n",
      "Confusion [rows y_true 0/1, cols y_hat 0/1]:\n",
      " [[ 661   57]\n",
      " [1776  416]]\n",
      "{'precision': np.float64(0.879), 'recall': np.float64(0.19), 'f1': np.float64(0.312), 'specificity': np.float64(0.921), 'tp': 416, 'fp': 57, 'fn': 1776, 'tn': 661, 'flagged': 473, 'n': 2910, 'flagged_rate': 0.163} | base_rate= 0.753 | AP= 0.776 | ROC= 0.555 | Brier= 0.187\n",
      "Saved flagged: outputs_b_section/flagged_test_T45_manual_thr0.900.lgb.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>specificity</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tn</th>\n",
       "      <th>flagged</th>\n",
       "      <th>n</th>\n",
       "      <th>flagged_rate</th>\n",
       "      <th>base_rate</th>\n",
       "      <th>ap_test</th>\n",
       "      <th>roc_test</th>\n",
       "      <th>brier_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>lgb</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.8127</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.9185</td>\n",
       "      <td>564</td>\n",
       "      <td>130</td>\n",
       "      <td>809</td>\n",
       "      <td>1465</td>\n",
       "      <td>694</td>\n",
       "      <td>2968</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>0.4626</td>\n",
       "      <td>0.7548</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>lgb</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.7986</td>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.8017</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>1166</td>\n",
       "      <td>294</td>\n",
       "      <td>283</td>\n",
       "      <td>713</td>\n",
       "      <td>1460</td>\n",
       "      <td>2456</td>\n",
       "      <td>0.5945</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.8548</td>\n",
       "      <td>0.8292</td>\n",
       "      <td>0.1733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>lgb</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>0.6994</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.7466</td>\n",
       "      <td>2331</td>\n",
       "      <td>376</td>\n",
       "      <td>1002</td>\n",
       "      <td>1108</td>\n",
       "      <td>2707</td>\n",
       "      <td>4817</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>lgb</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>416</td>\n",
       "      <td>57</td>\n",
       "      <td>1776</td>\n",
       "      <td>661</td>\n",
       "      <td>473</td>\n",
       "      <td>2910</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.7533</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.1869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    T model  threshold  precision  recall      f1  specificity    tp   fp  \\\n",
       "0   7   lgb       0.48     0.8127  0.4108  0.5457       0.9185   564  130   \n",
       "1  14   lgb       0.49     0.7986  0.8047  0.8017       0.7080  1166  294   \n",
       "2  30   lgb       0.63     0.8611  0.6994  0.7719       0.7466  2331  376   \n",
       "3  45   lgb       0.90     0.8795  0.1898  0.3122       0.9206   416   57   \n",
       "\n",
       "     fn    tn  flagged     n  flagged_rate  base_rate  ap_test  roc_test  \\\n",
       "0   809  1465      694  2968        0.2338     0.4626   0.7548    0.7977   \n",
       "1   283   713     1460  2456        0.5945     0.5900   0.8548    0.8292   \n",
       "2  1002  1108     2707  4817        0.5620     0.6919   0.8290    0.7422   \n",
       "3  1776   661      473  2910        0.1625     0.7533   0.7762    0.5547   \n",
       "\n",
       "   brier_test  \n",
       "0      0.1949  \n",
       "1      0.1733  \n",
       "2      0.1821  \n",
       "3      0.1869  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: outputs_b_section/thresholds_operational.json\n"
     ]
    }
   ],
   "source": [
    "# === Manual thresholds per horizon (\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, brier_score_loss\n",
    "\n",
    "# manual thresholds per horizon (tune based on business needs)\n",
    "HORIZONS   = [7, 14, 30, 45]\n",
    "MANUAL_THR = {7: 0.48, 14: 0.49, 30: 0.63, 45: 0.9}  \n",
    "\n",
    "#directories\n",
    "DIRS_TRY   = [Path('outputs_b_section'),\n",
    "              Path('outputs_b_section')]\n",
    "PRED_DIR   = next((d for d in DIRS_TRY if d.exists()), DIRS_TRY[-1])\n",
    "OUT_DIR    = Path('outputs_b_section'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _load_preds(T: int):\n",
    "    # Try CatBoost file first, then LightGBM\n",
    "    cat = PRED_DIR / f'p_hat_test_T{T}.cat.csv'\n",
    "    lgb = PRED_DIR / f'p_hat_test_T{T}.csv'\n",
    "    if cat.exists():\n",
    "        df = pd.read_csv(cat)\n",
    "        y = df['y_test'].astype(int).to_numpy()\n",
    "        p = df['p_test'].astype(float).to_numpy()\n",
    "        src = 'cat'\n",
    "    elif lgb.exists():\n",
    "        df = pd.read_csv(lgb)\n",
    "        y = df['y_test'].astype(int).to_numpy()\n",
    "        p = df['p_test'].astype(float).to_numpy()\n",
    "        src = 'lgb'\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No predictions found for T={T} in {PRED_DIR}\")\n",
    "    return y, p, df, src\n",
    "\n",
    "def _month_split(df_sub, time_col='t'):\n",
    "    mkey = df_sub[time_col].dt.tz_convert('UTC').dt.strftime('%Y-%m')\n",
    "    months = sorted(mkey.unique())\n",
    "    if len(months) < 3:\n",
    "        tmin, tmax = df_sub[time_col].min(), df_sub[time_col].max()\n",
    "        cut1 = tmin + (tmax - tmin)*0.6\n",
    "        cut2 = tmin + (tmax - tmin)*0.8\n",
    "        tr = df_sub[df_sub[time_col] < cut1]\n",
    "        va = df_sub[(df_sub[time_col] >= cut1) & (df_sub[time_col] < cut2)]\n",
    "        te = df_sub[df_sub[time_col] >= cut2]\n",
    "        return tr, va, te\n",
    "    test_m = months[-1]; val_m = months[-2]; train_m = months[:-2]\n",
    "    tr = df_sub[mkey.isin(train_m)]\n",
    "    va = df_sub[mkey == val_m]\n",
    "    te = df_sub[mkey == test_m]\n",
    "    return tr, va, te\n",
    "\n",
    "def _confusion_at(y, p, thr):\n",
    "    yhat = (p >= thr).astype(int)\n",
    "    cm = confusion_matrix(y, yhat, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec+1e-9) if (prec+rec)>0 else 0.0\n",
    "    spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "    return cm, dict(precision=prec, recall=rec, f1=f1, specificity=spec,\n",
    "                    tp=int(tp), fp=int(fp), fn=int(fn), tn=int(tn),\n",
    "                    flagged=int(yhat.sum()), n=len(y), flagged_rate=float((yhat==1).mean()))\n",
    "\n",
    "rows = []\n",
    "for T in HORIZONS:\n",
    "    thr = float(MANUAL_THR[T])\n",
    "    y, p, _pred, src = _load_preds(T)\n",
    "\n",
    "    # attach ids & times if labels[T] is present in the notebook\n",
    "    try:\n",
    "        sub = labels[T].copy()\n",
    "        _, _, te = _month_split(sub, 't')\n",
    "        te = te.reset_index(drop=True)\n",
    "        if len(te) != len(y):\n",
    "            print(f\"[warn] T={T}: test slice length {len(te)} != preds {len(y)}; writing without ids/t.\")\n",
    "            te = None\n",
    "    except Exception:\n",
    "        te = None\n",
    "\n",
    "    cm, m = _confusion_at(y, p, thr)\n",
    "    ap = float(average_precision_score(y, p))\n",
    "    roc = float(roc_auc_score(y, p)) if np.unique(y).size==2 else np.nan\n",
    "    brier = float(brier_score_loss(y, p))\n",
    "\n",
    "    print(f\"\\n=== T={T} | model={src} | manual thr={thr:.3f} ===\")\n",
    "    print(\"Confusion [rows y_true 0/1, cols y_hat 0/1]:\\n\", cm)\n",
    "    print({k: (round(v,3) if isinstance(v,float) else v) for k,v in m.items()},\n",
    "          \"| base_rate=\", round(y.mean(),3), \"| AP=\", round(ap,3),\n",
    "          \"| ROC=\", (round(roc,3) if roc==roc else 'nan'), \"| Brier=\", round(brier,3))\n",
    "\n",
    "    # save flagged list\n",
    "    if te is not None:\n",
    "        out_df = te[['user_id','t']].copy()\n",
    "    else:\n",
    "        out_df = pd.DataFrame({'user_id': np.nan, 't': np.nan}, index=range(len(y)))\n",
    "    out_df['p'] = p\n",
    "    out_df['y'] = y\n",
    "    out_df['flag'] = (p >= thr).astype(int)\n",
    "    out_path = OUT_DIR / f'flagged_test_T{T}_manual_thr{thr:.3f}.{src}.csv'\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(\"Saved flagged:\", out_path)\n",
    "\n",
    "    rows.append({'T':T, 'model':src, 'threshold':thr, **m,\n",
    "                 'base_rate': float(y.mean()), 'ap_test': ap, 'roc_test': roc, 'brier_test': brier})\n",
    "\n",
    "# summary & persist thresholds (as global)\n",
    "summary = pd.DataFrame(rows).sort_values('T')\n",
    "display(summary.round(4))\n",
    "\n",
    "(PRED_DIR/'thresholds_operational.json').write_text(json.dumps(\n",
    "    {'global': {str(T): float(MANUAL_THR[T]) for T in HORIZONS},\n",
    "     'note': 'manual thresholds chosen by analyst'},\n",
    "    indent=2\n",
    "))\n",
    "print(\"Updated:\", PRED_DIR/'thresholds_operational.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
