{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09819de",
   "metadata": {},
   "source": [
    "\n",
    "# Repurchase & Next-Purchase — v3 (No Quality Checks)\n",
    "\n",
    "This version **removes all quality checks** (no status/amount filters, no burst-dedup).  \n",
    "It keeps:\n",
    "\n",
    "- **Extended taxonomy** (Walk/ Sitting + WalkAndCare, Grooming; case-insensitive aliases)\n",
    "- **Repurchase & inter-purchase intervals**\n",
    "- **Next-purchase predictions** (recency-weighted transitions + personalized shrinkage)\n",
    "- **Time-based model validation** (Top-1 / Hit@K / MRR@K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da9b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Where your CSVs live\n",
    "DATA_DIR = Path('/Users/tree/Projects/tubitak-ai-agent/tubitakaiagentprojeleriiinverisetleri')  # e.g., Path('/mnt/data')\n",
    "\n",
    "# File/column assumptions\n",
    "PURCHASE_FILE = 'Purchase.csv'\n",
    "TIME_COL = 'ordercreatedtime'\n",
    "USER_ID_CANDIDATES = ['ownerid','user_id','id']\n",
    "SERVICETYPE_CANDS = ['serviceType','servicetype','service_type']\n",
    "\n",
    "# Extended taxonomy (lowercased)\n",
    "WALK_SET = {\n",
    "    'adhoc','planned','package','customize','walking','walk',\n",
    "    'walkandcare','walk & care','walk_and_care'\n",
    "}\n",
    "SITTING_SET = {\n",
    "    'boarding','sitting','catboarding','catsitting'\n",
    "}\n",
    "OTHER_KNOWN = {'grooming'}  # will map to 'Other' category\n",
    "\n",
    "# Modeling knobs\n",
    "CHURN_THRESHOLD_DAYS = 90\n",
    "HALFLIFE_DAYS = 90          # recency half-life for transition weights\n",
    "PERSONALIZATION_M = 5       # shrinkage toward global\n",
    "TOPK = 3                    # top-k predictions\n",
    "\n",
    "# Validation split\n",
    "HOLDOUT_DAYS = 30           # last 30 days = test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cf08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def coalesce_id(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "\n",
    "def parse_time_utc(s, local_tz='Europe/Istanbul'):\n",
    "    ts = pd.to_datetime(s, errors='coerce', utc=True, infer_datetime_format=True)\n",
    "    if getattr(ts.dt, 'tz', None) is None:\n",
    "        ts = pd.to_datetime(s, errors='coerce')\n",
    "        ts = ts.dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT').dt.tz_convert('UTC')\n",
    "    return ts\n",
    "\n",
    "def norm_service(series: pd.Series) -> pd.Series:\n",
    "    out = series.astype(str).str.strip().str.lower()\n",
    "    out = out.replace({'nan': np.nan, 'none': np.nan, 'nat': np.nan, '': np.nan})\n",
    "    out = out.replace({'walk & care': 'walkandcare', 'walk_and_care': 'walkandcare'})\n",
    "    return out\n",
    "\n",
    "def map_category(st: str) -> str:\n",
    "    if pd.isna(st): return np.nan\n",
    "    s = str(st).lower()\n",
    "    if s in WALK_SET: return 'Walk'\n",
    "    if s in SITTING_SET: return 'Sitting'\n",
    "    return 'Other'\n",
    "\n",
    "def days_between(t1, t0):\n",
    "    return (t1 - t0).dt.total_seconds()/(24*3600)\n",
    "\n",
    "def recency_weight(age_days, halflife_days):\n",
    "    return np.power(0.5, np.clip(age_days, 0, None) / max(halflife_days, 1e-9))\n",
    "\n",
    "def excel_safe(df, to_tz='Europe/Istanbul'):\n",
    "    out = df.copy()\n",
    "    for col in out.columns:\n",
    "        if pd.api.types.is_datetime64tz_dtype(out[col]):\n",
    "            out[col] = out[col].dt.tz_convert(to_tz).dt.tz_localize(None)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d70a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 28701 | Users: 1614 | Time range: 2024-12-24 21:41:02.022000+00:00 → 2025-08-04 09:34:52.801000+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5897/2720229937.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  ts = pd.to_datetime(s, errors='coerce', utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serviceid</th>\n",
       "      <th>ownerid</th>\n",
       "      <th>ordercreatedtime</th>\n",
       "      <th>servicetype</th>\n",
       "      <th>user_id</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>serviceType_raw</th>\n",
       "      <th>serviceType</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d2ca43d6-737f-4bd6-9482-3149127453da</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-14 18:20:44.706000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-14 18:20:44.706000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>adhoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3a2b4eeb-7366-4ab8-b64f-da683dc22a6e</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-15 18:58:14.754000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-15 18:58:14.754000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>adhoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a88a44d-7e9b-4c58-9eec-44e7c50d3d0b</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-05-23 18:28:44.253000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-05-23 18:28:44.253000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>adhoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77dfb189-b068-4034-a94d-9f62db799930</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-11 17:34:34.581000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-11 17:34:34.581000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>adhoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e3f614f-dde4-49e9-b297-f295033dc2c4</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-24 17:06:10.411000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-24 17:06:10.411000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>adhoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              serviceid                               ownerid  \\\n",
       "0  d2ca43d6-737f-4bd6-9482-3149127453da  005fc863-7c9c-42ce-af56-38fed3c545f3   \n",
       "1  3a2b4eeb-7366-4ab8-b64f-da683dc22a6e  005fc863-7c9c-42ce-af56-38fed3c545f3   \n",
       "2  2a88a44d-7e9b-4c58-9eec-44e7c50d3d0b  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "3  77dfb189-b068-4034-a94d-9f62db799930  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "4  0e3f614f-dde4-49e9-b297-f295033dc2c4  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "\n",
       "             ordercreatedtime servicetype  \\\n",
       "0  2025-05-14 18:20:44.706000       AdHoc   \n",
       "1  2025-05-15 18:58:14.754000       AdHoc   \n",
       "2  2025-05-23 18:28:44.253000       AdHoc   \n",
       "3  2025-06-11 17:34:34.581000       AdHoc   \n",
       "4  2025-06-24 17:06:10.411000       AdHoc   \n",
       "\n",
       "                                user_id                    purchase_time  \\\n",
       "0  005fc863-7c9c-42ce-af56-38fed3c545f3 2025-05-14 18:20:44.706000+00:00   \n",
       "1  005fc863-7c9c-42ce-af56-38fed3c545f3 2025-05-15 18:58:14.754000+00:00   \n",
       "2  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-05-23 18:28:44.253000+00:00   \n",
       "3  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-06-11 17:34:34.581000+00:00   \n",
       "4  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-06-24 17:06:10.411000+00:00   \n",
       "\n",
       "  serviceType_raw serviceType category  \n",
       "0           AdHoc       adhoc     Walk  \n",
       "1           AdHoc       adhoc     Walk  \n",
       "2           AdHoc       adhoc     Walk  \n",
       "3           AdHoc       adhoc     Walk  \n",
       "4           AdHoc       adhoc     Walk  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load purchases (no quality filters) ===\n",
    "from pathlib import Path\n",
    "\n",
    "p = pd.read_csv(Path(DATA_DIR) / PURCHASE_FILE)\n",
    "\n",
    "# coalesce id + time\n",
    "p['user_id'] = coalesce_id(p, USER_ID_CANDIDATES).astype(str).replace({'nan': np.nan})\n",
    "if TIME_COL not in p.columns:\n",
    "    raise KeyError(f\"Time column '{TIME_COL}' not found in {PURCHASE_FILE}\")\n",
    "p['purchase_time'] = parse_time_utc(p[TIME_COL])\n",
    "\n",
    "# serviceType normalize & categorize\n",
    "st_col = None\n",
    "for c in SERVICETYPE_CANDS:\n",
    "    if c in p.columns:\n",
    "        st_col = c; break\n",
    "p['serviceType_raw'] = p[st_col] if st_col else np.nan\n",
    "p['serviceType'] = norm_service(p['serviceType_raw'])\n",
    "p['category'] = p['serviceType'].apply(map_category)\n",
    "\n",
    "# drop missing criticals\n",
    "p = p.dropna(subset=['user_id','purchase_time']).copy()\n",
    "p['user_id'] = p['user_id'].astype(str)\n",
    "\n",
    "# sort\n",
    "p = p.sort_values(['user_id','purchase_time']).reset_index(drop=True)\n",
    "\n",
    "print('Rows:', len(p), '| Users:', p['user_id'].nunique(), '| Time range:', p['purchase_time'].min(), '→', p['purchase_time'].max())\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1efbf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   buyers  repeat_buyers  repurchase_rate\n",
       " 0    1614           1279         0.792441,\n",
       "                     scope    count  mean_days  median_days  p25_days  \\\n",
       " 0           all_intervals  27087.0   2.022260     0.000000  0.000000   \n",
       " 1  last_interval_per_user   1279.0   6.682605     1.160648  0.000841   \n",
       " \n",
       "    p75_days  min_days    max_days  \n",
       " 0  1.023671       0.0  137.020692  \n",
       " 1  7.860352       0.0   84.976695  )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Repurchase & intervals (raw) ===\n",
    "user_counts = p.groupby('user_id').size().rename('purchase_count')\n",
    "buyers = int((user_counts >= 1).sum())\n",
    "repeat_buyers = int((user_counts >= 2).sum())\n",
    "repurchase_rate = repeat_buyers / buyers if buyers else np.nan\n",
    "repurchase_summary = pd.DataFrame([{'buyers': buyers, 'repeat_buyers': repeat_buyers, 'repurchase_rate': repurchase_rate}])\n",
    "\n",
    "p['prev_time'] = p.groupby('user_id')['purchase_time'].shift(1)\n",
    "p['delta_days'] = (p['purchase_time'] - p['prev_time']).dt.total_seconds()/(24*3600)\n",
    "intervals = p.dropna(subset=['delta_days']).copy()\n",
    "\n",
    "def statblock(s):\n",
    "    s = s.dropna()\n",
    "    return pd.Series({\n",
    "        'count': int(s.size),\n",
    "        'mean_days': float(s.mean()) if s.size else np.nan,\n",
    "        'median_days': float(s.median()) if s.size else np.nan,\n",
    "        'p25_days': float(s.quantile(0.25)) if s.size else np.nan,\n",
    "        'p75_days': float(s.quantile(0.75)) if s.size else np.nan,\n",
    "        'min_days': float(s.min()) if s.size else np.nan,\n",
    "        'max_days': float(s.max()) if s.size else np.nan,\n",
    "    })\n",
    "\n",
    "overall_interval_stats = statblock(intervals['delta_days']).to_frame().T\n",
    "overall_interval_stats.insert(0, 'scope', 'all_intervals')\n",
    "\n",
    "last_purchase = p.groupby('user_id').tail(1).copy()\n",
    "second_last = p.groupby('user_id').nth(-2).reset_index()\n",
    "last_interval = last_purchase.merge(second_last[['user_id','purchase_time']].rename(columns={'purchase_time':'prev_purchase_time'}),\n",
    "                                    on='user_id', how='left')\n",
    "last_interval['last_delta_days'] = (last_interval['purchase_time'] - last_interval['prev_purchase_time']).dt.total_seconds()/(24*3600)\n",
    "last_interval = last_interval.dropna(subset=['last_delta_days'])\n",
    "last_interval_stats = statblock(last_interval['last_delta_days']).to_frame().T\n",
    "last_interval_stats.insert(0, 'scope', 'last_interval_per_user')\n",
    "\n",
    "interval_stats = pd.concat([overall_interval_stats, last_interval_stats], ignore_index=True)\n",
    "\n",
    "repurchase_summary, interval_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24748b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: 2024-12-24 21:41:02.022000+00:00 → 2025-07-05 09:33:34.610000+00:00\n",
      "Test  range: 2025-07-05 09:52:18.412000+00:00 → 2025-08-04 09:34:52.801000+00:00\n"
     ]
    }
   ],
   "source": [
    "# === Train/Test split & transitions ===\n",
    "max_time = p['purchase_time'].max()\n",
    "train_end = max_time - pd.Timedelta(days=HOLDOUT_DAYS)\n",
    "train = p[p['purchase_time'] <= train_end].copy()\n",
    "test  = p[p['purchase_time'] >  train_end].copy()\n",
    "\n",
    "print(\"Train range:\", train['purchase_time'].min(), \"→\", train['purchase_time'].max())\n",
    "print(\"Test  range:\", test['purchase_time'].min(), \"→\", test['purchase_time'].max())\n",
    "\n",
    "def make_transitions(df):\n",
    "    d = df.copy()\n",
    "    d['next_service'] = d.groupby('user_id')['serviceType'].shift(-1)\n",
    "    d['this_time'] = d['purchase_time']\n",
    "    d['next_time'] = d.groupby('user_id')['purchase_time'].shift(-1)\n",
    "    return d.dropna(subset=['serviceType','next_service','this_time','next_time']).copy()\n",
    "\n",
    "train_t = make_transitions(train)\n",
    "test_t  = make_transitions(test)\n",
    "\n",
    "# Recency-weighted prior from TRAIN\n",
    "train_t['age_days'] = (train['purchase_time'].max() - train_t['this_time']).dt.total_seconds()/(24*3600)\n",
    "train_t['w'] = recency_weight(train_t['age_days'], HALFLIFE_DAYS)\n",
    "rw_counts = (train_t.groupby(['serviceType','next_service'])['w'].sum()\n",
    "             .rename('w_count').reset_index())\n",
    "rw_totals = rw_counts.groupby('serviceType')['w_count'].transform('sum')\n",
    "rw_counts['prob'] = np.where(rw_totals > 0, rw_counts['w_count'] / rw_totals, np.nan)\n",
    "\n",
    "# User-specific counts from TRAIN (recency vs user's last train time)\n",
    "last_time_user = train.groupby('user_id')['purchase_time'].max().to_dict()\n",
    "d = train_t.copy()\n",
    "d['age_days_user'] = d.apply(lambda r: (last_time_user.get(r['user_id']) - r['this_time']).total_seconds()/(24*3600), axis=1)\n",
    "d['uw'] = recency_weight(d['age_days_user'], HALFLIFE_DAYS)\n",
    "\n",
    "from collections import defaultdict\n",
    "user_train_counts = {}\n",
    "for (uid, cur), grp in d.groupby(['user_id','serviceType']):\n",
    "    user_train_counts[(uid, cur)] = grp.groupby('next_service')['uw'].sum().to_dict()\n",
    "\n",
    "# Global fallback list (most common in TRAIN)\n",
    "global_order = train['serviceType'].value_counts(dropna=True).index.tolist()\n",
    "\n",
    "glob_prior = defaultdict(dict)\n",
    "for _, r in rw_counts.iterrows():\n",
    "    glob_prior[r['serviceType']][r['next_service']] = r['prob']\n",
    "if not glob_prior:\n",
    "    tmp = train_t.groupby(['serviceType','next_service']).size().rename('count').reset_index()\n",
    "    tot = tmp.groupby('serviceType')['count'].transform('sum')\n",
    "    tmp['prob'] = tmp['count'] / tot\n",
    "    for _, r in tmp.iterrows():\n",
    "        glob_prior[r['serviceType']][r['next_service']] = r['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9830ff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  metric     value\n",
       " 0   top1  0.879590\n",
       " 1  hit@3  0.992801\n",
       " 2  mrr@3  0.933855,\n",
       "   metric     value\n",
       " 0   top1  0.438625\n",
       " 1  hit@3  0.940425\n",
       " 2  mrr@3  0.656857)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Prediction & Validation ===\n",
    "def predict_next(current_service, user_id, m=PERSONALIZATION_M, topk=TOPK):\n",
    "    prior = glob_prior.get(current_service, {})\n",
    "    u = user_train_counts.get((user_id, current_service), {})\n",
    "    sum_u = sum(u.values()) if u else 0.0\n",
    "    cand = set(prior.keys()) | set(u.keys()) | set(global_order[:topk])\n",
    "    scores = []\n",
    "    for nxt in cand:\n",
    "        pu = u.get(nxt, 0.0)\n",
    "        pg = prior.get(nxt, np.nan)\n",
    "        if np.isnan(pg):\n",
    "            pg = 1.0 / max(len(cand), 1)  # uniform fallback\n",
    "        post = (pu + m * pg) / (sum_u + m if (sum_u + m) > 0 else 1.0)\n",
    "        scores.append((nxt, float(post)))\n",
    "    ranked = sorted(scores, key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return ranked\n",
    "\n",
    "def eval_metrics(test_t, topk=TOPK):\n",
    "    rows = []\n",
    "    for _, r in test_t.iterrows():\n",
    "        cur, nxt, uid = r['serviceType'], r['next_service'], r['user_id']\n",
    "        ranked = predict_next(cur, uid, topk=topk)\n",
    "        pred_list = [s for s,_ in ranked]\n",
    "        top1 = 1.0 if (len(pred_list)>0 and pred_list[0]==nxt) else 0.0\n",
    "        hitk = 1.0 if nxt in pred_list else 0.0\n",
    "        rr = 0.0\n",
    "        if nxt in pred_list:\n",
    "            rr = 1.0 / (pred_list.index(nxt) + 1)\n",
    "        rows.append({'current': cur, 'actual_next': nxt, 'top1': top1, f'hit@{topk}': hitk, f'mrr@{topk}': rr})\n",
    "    df = pd.DataFrame(rows)\n",
    "    overall = df[['top1', f'hit@{topk}', f'mrr@{topk}']].mean().to_frame('value').reset_index().rename(columns={'index':'metric'})\n",
    "    by_cur = df.groupby('current')[['top1', f'hit@{topk}', f'mrr@{topk}']].mean().reset_index()\n",
    "    return df, overall, by_cur\n",
    "\n",
    "test_preds, overall_metrics, by_current = eval_metrics(test_t, topk=TOPK)\n",
    "\n",
    "# Simple baseline (global top-k from TRAIN)\n",
    "def baseline_eval(test_t, topk=TOPK):\n",
    "    base_list = global_order[:topk]\n",
    "    rows = []\n",
    "    for _, r in test_t.iterrows():\n",
    "        nxt = r['next_service']\n",
    "        top1 = 1.0 if (len(base_list) > 0 and base_list[0] == nxt) else 0.0\n",
    "        hitk = 1.0 if nxt in base_list else 0.0\n",
    "        rr = 0.0\n",
    "        if nxt in base_list:\n",
    "            rr = 1.0 / (base_list.index(nxt) + 1)\n",
    "        rows.append({'top1': top1, f'hit@{topk}': hitk, f'mrr@{topk}': rr})\n",
    "    df = pd.DataFrame(rows)\n",
    "    overall = df.mean().to_frame('value').reset_index().rename(columns={'index':'metric'})\n",
    "    return overall\n",
    "\n",
    "baseline_metrics = baseline_eval(test_t, topk=TOPK)\n",
    "\n",
    "overall_metrics, baseline_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166f8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5897/2720229937.py:40: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(out[col]):\n",
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5897/2720229937.py:40: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(out[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel report: /Users/tree/Projects/tubitak-ai-agent/tubitakaiagentprojeleriiinverisetleri/repurchase_outputs_v3/repurchase_validation_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Save outputs ===\n",
    "OUT_DIR = Path(DATA_DIR) / 'repurchase_outputs_v3'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "excel_engine = None\n",
    "try:\n",
    "    import xlsxwriter  # noqa\n",
    "    excel_engine = 'xlsxwriter'\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openpyxl  # noqa\n",
    "        excel_engine = 'openpyxl'\n",
    "    except ImportError:\n",
    "        excel_engine = None\n",
    "\n",
    "# CSVs\n",
    "excel_safe(repurchase_summary).to_csv(OUT_DIR / 'repurchase_summary.csv', index=False)\n",
    "excel_safe(intervals[['user_id','prev_time','purchase_time','delta_days','serviceType','category']]).to_csv(OUT_DIR / 'interpurchase_intervals.csv', index=False)\n",
    "excel_safe(pd.concat([overall_interval_stats, last_interval_stats], ignore_index=True)).to_csv(OUT_DIR / 'interpurchase_interval_stats.csv', index=False)\n",
    "excel_safe(rw_counts).to_csv(OUT_DIR / 'train_recency_weighted_transitions.csv', index=False)\n",
    "excel_safe(test_preds).to_csv(OUT_DIR / 'validation_predictions_detail.csv', index=False)\n",
    "excel_safe(overall_metrics).to_csv(OUT_DIR / 'validation_metrics_overall.csv', index=False)\n",
    "excel_safe(by_current).to_csv(OUT_DIR / 'validation_metrics_by_current.csv', index=False)\n",
    "\n",
    "# Excel pack (optional)\n",
    "if excel_engine:\n",
    "    xls = OUT_DIR / 'repurchase_validation_report.xlsx'\n",
    "    with pd.ExcelWriter(xls, engine=excel_engine) as writer:\n",
    "        excel_safe(repurchase_summary).to_excel(writer, sheet_name='01_repurchase', index=False)\n",
    "        excel_safe(pd.concat([overall_interval_stats, last_interval_stats], ignore_index=True)).to_excel(writer, sheet_name='02_intervals', index=False)\n",
    "        excel_safe(rw_counts).to_excel(writer, sheet_name='03_transitions_train', index=False)\n",
    "        excel_safe(overall_metrics).to_excel(writer, sheet_name='04_valid_overall', index=False)\n",
    "        excel_safe(by_current).to_excel(writer, sheet_name='05_valid_by_current', index=False)\n",
    "        excel_safe(test_preds.head(5000)).to_excel(writer, sheet_name='06_prediction_samples', index=False)\n",
    "    print(\"Excel report:\", xls)\n",
    "else:\n",
    "    print(\"No Excel engine detected; wrote CSVs only.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
