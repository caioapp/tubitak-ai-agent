{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39b6109",
   "metadata": {},
   "source": [
    "\n",
    "# Repurchase & Next-Purchase Intelligence\n",
    "\n",
    "This notebook builds on your purchase data to deliver:\n",
    "1. **Repurchase metrics** (repeat buyers, repurchase rate)\n",
    "2. **Inter-purchase intervals** (overall and last-interval per user)\n",
    "3. **Churn flag** per user (no repeat within *X* days)\n",
    "4. **Next-purchase predictions** at two levels:\n",
    "   - **ServiceType** (fine-grained)\n",
    "   - **Category** (Walk vs Sitting), derived from `serviceType`\n",
    "5. **Time-aware transitions** (recency-weighted, plus seasonal by month-of-year)\n",
    "6. **Personalized ranking** that blends a user's own history with global patterns via shrinkage\n",
    "\n",
    "> **Parameters you can tweak** live in the next cell (e.g., churn threshold, half-life for recency weighting, top-K predictions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bde1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Where your CSVs live\n",
    "DATA_DIR = Path('/Users/tree/Projects/tubitak-ai-agent/tubitakaiagentprojeleriiinverisetleri')  # change if needed, e.g., Path('/mnt/data')\n",
    "\n",
    "# File & field assumptions\n",
    "PURCHASE_FILE = 'Purchase.csv'           # has columns: ordercreatedtime, ownerid/user_id/id, serviceType\n",
    "TIME_COL = 'ordercreatedtime'            # purchase timestamp\n",
    "USER_ID_CANDIDATES = ['ownerid','user_id','id']\n",
    "SERVICETYPE_CANDS = ['serviceType','servicetype','service_type']\n",
    "\n",
    "# Category taxonomy\n",
    "WALK_TYPES    = {'AdHoc','Planned','Package','Customize','Walking'}\n",
    "SITTING_TYPES = {'Boarding','Sitting','CatBoarding','CatSitting'}\n",
    "\n",
    "# Modeling knobs\n",
    "CHURN_THRESHOLD_DAYS = 90         # flag churn risk if no repeat within this many days\n",
    "HALFLIFE_DAYS = 90                # recency half-life for weighting transitions (larger -> slower decay)\n",
    "PERSONALIZATION_M = 5             # shrinkage strength toward global distribution (higher = more global)\n",
    "TOPK = 3                          # top-K predicted serviceTypes to output\n",
    "\n",
    "# Seasonal model toggle (month-of-year conditioning). Keep True for diagnostics; predictions use recency-weighted + personalization.\n",
    "USE_SEASONAL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be28e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20443023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceb5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the .gitignore first\n",
    "git add .gitignore\n",
    "\n",
    "# Add the notebooks\n",
    "git add notebooks/funnel_analysis.ipynb\n",
    "git add notebooks/repurchase_next_purchase.ipynb\n",
    "\n",
    "# Commit the changes\n",
    "git commit -m \"add funnel analysis and repurchase notebooks\"\n",
    "\n",
    "# Push to remote\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c183baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def coalesce_id(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "\n",
    "def parse_time_utc(s, local_tz='Europe/Istanbul'):\n",
    "    ts = pd.to_datetime(s, errors='coerce', utc=True, infer_datetime_format=True)\n",
    "    if getattr(ts.dt, 'tz', None) is None:\n",
    "        ts = pd.to_datetime(s, errors='coerce')  # naive\n",
    "        ts = ts.dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT').dt.tz_convert('UTC')\n",
    "    return ts\n",
    "\n",
    "def normalize_service(series: pd.Series) -> pd.Series:\n",
    "    out = series.astype(str).str.strip()\n",
    "    # treat placeholder strings as missing\n",
    "    out = out.replace({'nan': np.nan, 'None': np.nan, 'NaT': np.nan, '': np.nan})\n",
    "    return out\n",
    "\n",
    "def service_category(st: str) -> str:\n",
    "    if pd.isna(st):\n",
    "        return np.nan\n",
    "    s = str(st)\n",
    "    if s in WALK_TYPES:\n",
    "        return 'Walk'\n",
    "    if s in SITTING_TYPES:\n",
    "        return 'Sitting'\n",
    "    return 'Other'\n",
    "\n",
    "def season_month(ts):\n",
    "    return ts.dt.tz_convert('Europe/Istanbul').dt.month\n",
    "\n",
    "def days_between(t1, t0):\n",
    "    return (t1 - t0).dt.total_seconds() / (24*3600)\n",
    "\n",
    "def recency_weight(age_days, halflife_days):\n",
    "    # weight = 0.5^(age/halflife)\n",
    "    return np.power(0.5, np.clip(age_days, 0, None) / max(halflife_days, 1e-9))\n",
    "\n",
    "def excel_safe(df, to_tz='Europe/Istanbul'):\n",
    "    out = df.copy()\n",
    "    for col in out.columns:\n",
    "        if pd.api.types.is_datetime64tz_dtype(out[col]):\n",
    "            out[col] = out[col].dt.tz_convert(to_tz).dt.tz_localize(None)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443cc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded purchases: 28701 | Users: 1614\n",
      "Time range UTC: 2024-12-24 21:41:02.022000+00:00 â†’ 2025-08-04 09:34:52.801000+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5056/1059534697.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  ts = pd.to_datetime(s, errors='coerce', utc=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serviceid</th>\n",
       "      <th>ownerid</th>\n",
       "      <th>ordercreatedtime</th>\n",
       "      <th>servicetype</th>\n",
       "      <th>user_id</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>serviceType_raw</th>\n",
       "      <th>serviceType</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d2ca43d6-737f-4bd6-9482-3149127453da</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-14 18:20:44.706000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-14 18:20:44.706000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3a2b4eeb-7366-4ab8-b64f-da683dc22a6e</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-15 18:58:14.754000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-15 18:58:14.754000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a88a44d-7e9b-4c58-9eec-44e7c50d3d0b</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-05-23 18:28:44.253000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-05-23 18:28:44.253000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77dfb189-b068-4034-a94d-9f62db799930</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-11 17:34:34.581000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-11 17:34:34.581000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e3f614f-dde4-49e9-b297-f295033dc2c4</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-24 17:06:10.411000</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-24 17:06:10.411000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              serviceid                               ownerid  \\\n",
       "0  d2ca43d6-737f-4bd6-9482-3149127453da  005fc863-7c9c-42ce-af56-38fed3c545f3   \n",
       "1  3a2b4eeb-7366-4ab8-b64f-da683dc22a6e  005fc863-7c9c-42ce-af56-38fed3c545f3   \n",
       "2  2a88a44d-7e9b-4c58-9eec-44e7c50d3d0b  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "3  77dfb189-b068-4034-a94d-9f62db799930  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "4  0e3f614f-dde4-49e9-b297-f295033dc2c4  00bc3aed-8e44-4a3f-8a87-c24e9c72106f   \n",
       "\n",
       "             ordercreatedtime servicetype  \\\n",
       "0  2025-05-14 18:20:44.706000       AdHoc   \n",
       "1  2025-05-15 18:58:14.754000       AdHoc   \n",
       "2  2025-05-23 18:28:44.253000       AdHoc   \n",
       "3  2025-06-11 17:34:34.581000       AdHoc   \n",
       "4  2025-06-24 17:06:10.411000       AdHoc   \n",
       "\n",
       "                                user_id                    purchase_time  \\\n",
       "0  005fc863-7c9c-42ce-af56-38fed3c545f3 2025-05-14 18:20:44.706000+00:00   \n",
       "1  005fc863-7c9c-42ce-af56-38fed3c545f3 2025-05-15 18:58:14.754000+00:00   \n",
       "2  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-05-23 18:28:44.253000+00:00   \n",
       "3  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-06-11 17:34:34.581000+00:00   \n",
       "4  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-06-24 17:06:10.411000+00:00   \n",
       "\n",
       "  serviceType_raw serviceType category  \n",
       "0           AdHoc       AdHoc     Walk  \n",
       "1           AdHoc       AdHoc     Walk  \n",
       "2           AdHoc       AdHoc     Walk  \n",
       "3           AdHoc       AdHoc     Walk  \n",
       "4           AdHoc       AdHoc     Walk  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load purchases ---\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "p_path = DATA_DIR / PURCHASE_FILE\n",
    "p = pd.read_csv(p_path)\n",
    "\n",
    "# Coalesce ids and timestamps\n",
    "p['user_id'] = coalesce_id(p, USER_ID_CANDIDATES).astype(str).replace({'nan': np.nan})\n",
    "if TIME_COL not in p.columns:\n",
    "    raise KeyError(f\"Time column '{TIME_COL}' not found in {PURCHASE_FILE}\")\n",
    "p['purchase_time'] = parse_time_utc(p[TIME_COL])\n",
    "\n",
    "# serviceType\n",
    "st_col = None\n",
    "for c in SERVICETYPE_CANDS:\n",
    "    if c in p.columns:\n",
    "        st_col = c\n",
    "        break\n",
    "p['serviceType_raw'] = p[st_col] if st_col else np.nan\n",
    "p['serviceType'] = normalize_service(p['serviceType_raw'])\n",
    "p['category'] = p['serviceType'].apply(service_category)\n",
    "\n",
    "# Drop invalid\n",
    "p = p.dropna(subset=['user_id','purchase_time']).copy()\n",
    "p['user_id'] = p['user_id'].astype(str)\n",
    "\n",
    "# Sort chronologically per user\n",
    "p = p.sort_values(['user_id','purchase_time']).reset_index(drop=True)\n",
    "\n",
    "max_time = p['purchase_time'].max()\n",
    "min_time = p['purchase_time'].min()\n",
    "print('Loaded purchases:', len(p), '| Users:', p['user_id'].nunique())\n",
    "print('Time range UTC:', min_time, 'â†’', max_time)\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e488d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   buyers  repeat_buyers  repurchase_rate\n",
       " 0    1614           1279         0.792441,\n",
       "                     scope    count  mean_days  median_days  p25_days  \\\n",
       " 0           all_intervals  27087.0   2.022260     0.000000  0.000000   \n",
       " 1  last_interval_per_user   1279.0   6.682605     1.160648  0.000841   \n",
       " \n",
       "    p75_days  min_days    max_days  \n",
       " 0  1.023671       0.0  137.020692  \n",
       " 1  7.860352       0.0   84.976695  )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Repurchase metrics & interpurchase intervals ---\n",
    "user_counts = p.groupby('user_id').size().rename('purchase_count')\n",
    "buyers = int((user_counts >= 1).sum())\n",
    "repeat_buyers = int((user_counts >= 2).sum())\n",
    "repurchase_rate = repeat_buyers / buyers if buyers else np.nan\n",
    "\n",
    "repurchase_summary = pd.DataFrame([{\n",
    "    'buyers': buyers,\n",
    "    'repeat_buyers': repeat_buyers,\n",
    "    'repurchase_rate': repurchase_rate\n",
    "}])\n",
    "\n",
    "# Interpurchase intervals\n",
    "p['prev_time'] = p.groupby('user_id')['purchase_time'].shift(1)\n",
    "p['delta_days'] = days_between(p['purchase_time'], p['prev_time'])\n",
    "intervals = p.dropna(subset=['delta_days']).copy()\n",
    "\n",
    "def statblock(s):\n",
    "    s = s.dropna()\n",
    "    return pd.Series({\n",
    "        'count': int(s.size),\n",
    "        'mean_days': float(s.mean()) if s.size else np.nan,\n",
    "        'median_days': float(s.median()) if s.size else np.nan,\n",
    "        'p25_days': float(s.quantile(0.25)) if s.size else np.nan,\n",
    "        'p75_days': float(s.quantile(0.75)) if s.size else np.nan,\n",
    "        'min_days': float(s.min()) if s.size else np.nan,\n",
    "        'max_days': float(s.max()) if s.size else np.nan,\n",
    "    })\n",
    "\n",
    "overall_interval_stats = statblock(intervals['delta_days']).to_frame().T\n",
    "overall_interval_stats.insert(0, 'scope', 'all_intervals')\n",
    "\n",
    "last_purchase = p.groupby('user_id').tail(1).copy()\n",
    "second_last = p.groupby('user_id').nth(-2).reset_index()\n",
    "last_interval = last_purchase.merge(\n",
    "    second_last[['user_id','purchase_time']].rename(columns={'purchase_time':'prev_purchase_time'}),\n",
    "    on='user_id', how='left'\n",
    ")\n",
    "last_interval['last_delta_days'] = days_between(last_interval['purchase_time'], last_interval['prev_purchase_time'])\n",
    "last_interval = last_interval.dropna(subset=['last_delta_days'])\n",
    "last_interval_stats = statblock(last_interval['last_delta_days']).to_frame().T\n",
    "last_interval_stats.insert(0, 'scope', 'last_interval_per_user')\n",
    "\n",
    "interval_stats = pd.concat([overall_interval_stats, last_interval_stats], ignore_index=True)\n",
    "\n",
    "repurchase_summary, interval_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d879af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn_risk</th>\n",
       "      <th>users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn_risk  users\n",
       "0       False   1590\n",
       "1        True     24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Churn flag (no repeat within X days) ---\n",
    "# For each user, compute days since last purchase relative to max_time in dataset\n",
    "last_by_user = p.groupby('user_id').tail(1).copy()\n",
    "last_by_user['days_since_last'] = days_between(max_time, last_by_user['purchase_time'])\n",
    "last_by_user['churn_risk'] = last_by_user['days_since_last'] > CHURN_THRESHOLD_DAYS\n",
    "\n",
    "churn_summary = last_by_user['churn_risk'].value_counts(dropna=False).rename_axis('churn_risk').to_frame('users').reset_index()\n",
    "churn_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd0d4380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serviceType</th>\n",
       "      <th>next_service</th>\n",
       "      <th>w_count</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>3849.470909</td>\n",
       "      <td>0.832128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Planned</td>\n",
       "      <td>698.303155</td>\n",
       "      <td>0.150950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Customize</td>\n",
       "      <td>52.005153</td>\n",
       "      <td>0.011242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Boarding</td>\n",
       "      <td>12.891097</td>\n",
       "      <td>0.002787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>7.536987</td>\n",
       "      <td>0.001629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>WalkAndCare</td>\n",
       "      <td>3.701048</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Grooming</td>\n",
       "      <td>2.149005</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Boarding</td>\n",
       "      <td>WalkAndCare</td>\n",
       "      <td>63.564595</td>\n",
       "      <td>0.589692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boarding</td>\n",
       "      <td>Boarding</td>\n",
       "      <td>22.843754</td>\n",
       "      <td>0.211923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Boarding</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>7.673942</td>\n",
       "      <td>0.071192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   serviceType next_service      w_count      prob\n",
       "0        AdHoc        AdHoc  3849.470909  0.832128\n",
       "4        AdHoc      Planned   698.303155  0.150950\n",
       "2        AdHoc    Customize    52.005153  0.011242\n",
       "1        AdHoc     Boarding    12.891097  0.002787\n",
       "5        AdHoc      Sitting     7.536987  0.001629\n",
       "6        AdHoc  WalkAndCare     3.701048  0.000800\n",
       "3        AdHoc     Grooming     2.149005  0.000465\n",
       "13    Boarding  WalkAndCare    63.564595  0.589692\n",
       "8     Boarding     Boarding    22.843754  0.211923\n",
       "7     Boarding        AdHoc     7.673942  0.071192"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Transition models ---\n",
    "# Build consecutive purchase transitions per user (serviceType), also season/age\n",
    "p['next_service'] = p.groupby('user_id')['serviceType'].shift(-1)\n",
    "p['this_time'] = p['purchase_time']\n",
    "p['next_time'] = p.groupby('user_id')['purchase_time'].shift(-1)\n",
    "trans = p.dropna(subset=['serviceType','next_service','this_time','next_time']).copy()\n",
    "\n",
    "# Global counts/probs\n",
    "glob_counts = (trans.groupby(['serviceType','next_service'])\n",
    "               .size().rename('count').reset_index())\n",
    "glob_totals = glob_counts.groupby('serviceType')['count'].transform('sum')\n",
    "glob_counts['prob'] = glob_counts['count'] / glob_totals\n",
    "\n",
    "# Category transitions (serviceType -> category of next)\n",
    "trans['next_category'] = trans['next_service'].apply(service_category)\n",
    "cat_counts = (trans.groupby(['category','next_category']).size().rename('count').reset_index())\n",
    "cat_totals = cat_counts.groupby('category')['count'].transform('sum')\n",
    "cat_counts['prob'] = cat_counts['count'] / cat_totals\n",
    "\n",
    "# Recency-weighted transitions (time-aware)\n",
    "# Weight each transition by age relative to dataset max_time\n",
    "trans['age_days'] = days_between(max_time, trans['this_time'])\n",
    "trans['w'] = recency_weight(trans['age_days'], HALFLIFE_DAYS)\n",
    "rw_counts = (trans.groupby(['serviceType','next_service'])['w'].sum()\n",
    "             .rename('w_count').reset_index())\n",
    "rw_totals = rw_counts.groupby('serviceType')['w_count'].transform('sum')\n",
    "rw_counts['prob'] = np.where(rw_totals > 0, rw_counts['w_count'] / rw_totals, np.nan)\n",
    "\n",
    "# Seasonal (by month-of-year)\n",
    "if USE_SEASONAL:\n",
    "    trans['month'] = season_month(trans['this_time'])\n",
    "    seas_counts = (trans.groupby(['serviceType','month','next_service']).size()\n",
    "                   .rename('count').reset_index())\n",
    "    # Convert to conditional probabilities by (serviceType, month)\n",
    "    seas_counts['total'] = seas_counts.groupby(['serviceType','month'])['count'].transform('sum')\n",
    "    seas_counts['prob'] = seas_counts['count'] / seas_counts['total']\n",
    "else:\n",
    "    seas_counts = pd.DataFrame(columns=['serviceType','month','next_service','count','total','prob'])\n",
    "\n",
    "rw_counts.sort_values(['serviceType','prob'], ascending=[True, False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f12cd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>last_purchase_time_utc</th>\n",
       "      <th>last_serviceType</th>\n",
       "      <th>last_category</th>\n",
       "      <th>predicted_top1_serviceType</th>\n",
       "      <th>predicted_top1_conf</th>\n",
       "      <th>predicted_topk</th>\n",
       "      <th>predicted_topk_conf</th>\n",
       "      <th>predicted_top1_category</th>\n",
       "      <th>expected_next_date_utc</th>\n",
       "      <th>days_since_last</th>\n",
       "      <th>churn_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005fc863-7c9c-42ce-af56-38fed3c545f3</td>\n",
       "      <td>2025-05-15 18:58:14.754000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>0.860106</td>\n",
       "      <td>[AdHoc, Planned, Customize]</td>\n",
       "      <td>[0.8601064962269586, 0.12579163022558515, 0.00...</td>\n",
       "      <td>Walk</td>\n",
       "      <td>2025-05-16 19:35:44.801999999+00:00</td>\n",
       "      <td>80.608774</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00bc3aed-8e44-4a3f-8a87-c24e9c72106f</td>\n",
       "      <td>2025-06-24 17:06:10.411000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>0.877718</td>\n",
       "      <td>[AdHoc, Planned, Customize]</td>\n",
       "      <td>[0.877717640908321, 0.10995576551523148, 0.008...</td>\n",
       "      <td>Walk</td>\n",
       "      <td>2025-07-10 16:24:53.490000+00:00</td>\n",
       "      <td>40.686602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00fc54bf-46d8-4328-8960-a40415005299</td>\n",
       "      <td>2025-06-17 16:29:28.280000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>0.879096</td>\n",
       "      <td>[AdHoc, Planned, Customize]</td>\n",
       "      <td>[0.8790963610663846, 0.10871602634486194, 0.00...</td>\n",
       "      <td>Walk</td>\n",
       "      <td>2025-07-05 15:35:18.397500+00:00</td>\n",
       "      <td>47.712089</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>015cdb4a-9f28-4d45-bc79-a374589bd648</td>\n",
       "      <td>2025-08-01 10:56:49.938000+00:00</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>Walk</td>\n",
       "      <td>AdHoc</td>\n",
       "      <td>0.687963</td>\n",
       "      <td>[AdHoc, Planned, Customize]</td>\n",
       "      <td>[0.6879626440127831, 0.2508750931787598, 0.059...</td>\n",
       "      <td>Walk</td>\n",
       "      <td>2025-08-01 10:56:49.938000+00:00</td>\n",
       "      <td>2.943089</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01a7af8b-0dea-45df-8eea-6c35682c359d</td>\n",
       "      <td>2025-07-19 11:38:31.598000+00:00</td>\n",
       "      <td>Customize</td>\n",
       "      <td>Walk</td>\n",
       "      <td>Customize</td>\n",
       "      <td>0.997701</td>\n",
       "      <td>[Customize, Planned, AdHoc]</td>\n",
       "      <td>[0.997700959130749, 0.0015069800184234956, 0.0...</td>\n",
       "      <td>Walk</td>\n",
       "      <td>2025-07-19 11:38:31.598000+00:00</td>\n",
       "      <td>15.914134</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id           last_purchase_time_utc  \\\n",
       "0  005fc863-7c9c-42ce-af56-38fed3c545f3 2025-05-15 18:58:14.754000+00:00   \n",
       "1  00bc3aed-8e44-4a3f-8a87-c24e9c72106f 2025-06-24 17:06:10.411000+00:00   \n",
       "2  00fc54bf-46d8-4328-8960-a40415005299 2025-06-17 16:29:28.280000+00:00   \n",
       "3  015cdb4a-9f28-4d45-bc79-a374589bd648 2025-08-01 10:56:49.938000+00:00   \n",
       "4  01a7af8b-0dea-45df-8eea-6c35682c359d 2025-07-19 11:38:31.598000+00:00   \n",
       "\n",
       "  last_serviceType last_category predicted_top1_serviceType  \\\n",
       "0            AdHoc          Walk                      AdHoc   \n",
       "1            AdHoc          Walk                      AdHoc   \n",
       "2            AdHoc          Walk                      AdHoc   \n",
       "3            AdHoc          Walk                      AdHoc   \n",
       "4        Customize          Walk                  Customize   \n",
       "\n",
       "   predicted_top1_conf               predicted_topk  \\\n",
       "0             0.860106  [AdHoc, Planned, Customize]   \n",
       "1             0.877718  [AdHoc, Planned, Customize]   \n",
       "2             0.879096  [AdHoc, Planned, Customize]   \n",
       "3             0.687963  [AdHoc, Planned, Customize]   \n",
       "4             0.997701  [Customize, Planned, AdHoc]   \n",
       "\n",
       "                                 predicted_topk_conf predicted_top1_category  \\\n",
       "0  [0.8601064962269586, 0.12579163022558515, 0.00...                    Walk   \n",
       "1  [0.877717640908321, 0.10995576551523148, 0.008...                    Walk   \n",
       "2  [0.8790963610663846, 0.10871602634486194, 0.00...                    Walk   \n",
       "3  [0.6879626440127831, 0.2508750931787598, 0.059...                    Walk   \n",
       "4  [0.997700959130749, 0.0015069800184234956, 0.0...                    Walk   \n",
       "\n",
       "               expected_next_date_utc  days_since_last  churn_risk  \n",
       "0 2025-05-16 19:35:44.801999999+00:00        80.608774       False  \n",
       "1    2025-07-10 16:24:53.490000+00:00        40.686602       False  \n",
       "2    2025-07-05 15:35:18.397500+00:00        47.712089       False  \n",
       "3    2025-08-01 10:56:49.938000+00:00         2.943089       False  \n",
       "4    2025-07-19 11:38:31.598000+00:00        15.914134       False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Personalized ranking (shrinkage to global) + predictions ---\n",
    "# We'll use recency-weighted globals as prior (rw_counts). For each user, compute their own last-k transitions from their last service.\n",
    "# Shrinkage: posterior p(next|last_svc, user) = (c_user(next) + m * p_global(next|last_svc)) / (sum_user + m)\n",
    "\n",
    "m = PERSONALIZATION_M\n",
    "\n",
    "# Build a global prior dict: for each current service, a mapping to next prob\n",
    "from collections import defaultdict\n",
    "\n",
    "glob_prior = defaultdict(dict)\n",
    "for _, r in rw_counts.iterrows():\n",
    "    glob_prior[r['serviceType']][r['next_service']] = r['prob']\n",
    "\n",
    "# If a current service has no recency-weighted prior, fall back to plain global\n",
    "if not glob_prior:\n",
    "    for _, r in glob_counts.iterrows():\n",
    "        glob_prior[r['serviceType']][r['next_service']] = r['prob']\n",
    "\n",
    "# User-specific last-k transitions from the SAME current service as their last purchase\n",
    "# (We weight the user's own transitions by recency as well)\n",
    "def user_transition_counts(df_user, last_service):\n",
    "    # consider transitions where current service == last_service\n",
    "    d = df_user[(df_user['serviceType'] == last_service) & (~df_user['next_service'].isna())].copy()\n",
    "    if d.empty:\n",
    "        return {}\n",
    "    d['age_days'] = days_between(df_user['purchase_time'].max(), d['this_time'])\n",
    "    d['w'] = recency_weight(d['age_days'], HALFLIFE_DAYS)\n",
    "    by_next = d.groupby('next_service')['w'].sum()\n",
    "    return by_next.to_dict()\n",
    "\n",
    "# Predict top-K serviceTypes per user\n",
    "last_txn = p.groupby('user_id').tail(1)[['user_id','serviceType','purchase_time','category']].rename(columns={'serviceType':'last_service'})\n",
    "pred_rows = []\n",
    "\n",
    "for _, row in last_txn.iterrows():\n",
    "    uid = row['user_id']\n",
    "    last_svc = row['last_service']\n",
    "    last_cat = row['category']\n",
    "    last_time = row['purchase_time']\n",
    "    dfu = trans[trans['user_id'] == uid] if 'user_id' in trans.columns else pd.DataFrame()\n",
    "    user_counts = user_transition_counts(dfu, last_svc) if not dfu.empty else {}\n",
    "    sum_user = sum(user_counts.values()) if user_counts else 0.0\n",
    "\n",
    "    prior = glob_prior.get(last_svc, {})\n",
    "    # Combine with shrinkage\n",
    "    next_candidates = set(prior.keys()) | set(user_counts.keys())\n",
    "    scored = []\n",
    "    for nxt in next_candidates:\n",
    "        pu = user_counts.get(nxt, 0.0)\n",
    "        pg = prior.get(nxt, np.nan)\n",
    "        # if prior missing, use uniform over observed next candidates\n",
    "        if np.isnan(pg):\n",
    "            pg = 1.0 / max(len(next_candidates), 1)\n",
    "        post = (pu + m * pg) / (sum_user + m if (sum_user + m) > 0 else 1.0)\n",
    "        scored.append((nxt, post))\n",
    "    if not scored:\n",
    "        # fallback to most common overall serviceType\n",
    "        most_common = p['serviceType'].value_counts(dropna=True)\n",
    "        if len(most_common):\n",
    "            nxt = most_common.index[0]\n",
    "            scored = [(nxt, 1.0)]\n",
    "        else:\n",
    "            scored = []\n",
    "    ranked = sorted(scored, key=lambda x: x[1], reverse=True)[:TOPK]\n",
    "\n",
    "    # Expected next date: use user's median interpurchase if available; else global median\n",
    "    user_ip = intervals[intervals['user_id']==uid]['delta_days']\n",
    "    if user_ip.notna().any():\n",
    "        median_days = float(user_ip.median())\n",
    "    else:\n",
    "        median_days = float(intervals['delta_days'].median()) if len(intervals) else np.nan\n",
    "    expected_next_date = pd.NaT\n",
    "    if not np.isnan(median_days):\n",
    "        expected_next_date = last_time + pd.Timedelta(days=median_days)\n",
    "\n",
    "    # Churn risk: if days since last > threshold\n",
    "    days_since = (max_time - last_time).total_seconds()/(24*3600)\n",
    "    churn_risk = days_since > CHURN_THRESHOLD_DAYS\n",
    "\n",
    "    pred_rows.append({\n",
    "        'user_id': uid,\n",
    "        'last_purchase_time_utc': last_time,\n",
    "        'last_serviceType': last_svc,\n",
    "        'last_category': last_cat,\n",
    "        'predicted_top1_serviceType': ranked[0][0] if ranked else np.nan,\n",
    "        'predicted_top1_conf': ranked[0][1] if ranked else np.nan,\n",
    "        'predicted_topk': [s for s,_ in ranked],\n",
    "        'predicted_topk_conf': [float(c) for _,c in ranked],\n",
    "        'predicted_top1_category': service_category(ranked[0][0]) if ranked else np.nan,\n",
    "        'expected_next_date_utc': expected_next_date,\n",
    "        'days_since_last': days_since,\n",
    "        'churn_risk': churn_risk\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fccc3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Seasonal diagnostic (optional) ---\n",
    "if USE_SEASONAL and not seas_counts.empty:\n",
    "    # For demonstration: show seasonal distribution for the most common current service\n",
    "    top_current = p['serviceType'].value_counts(dropna=True).index[0]\n",
    "    seas_demo = seas_counts[seas_counts['serviceType']==top_current].sort_values(['month','prob'], ascending=[True, False]).head(24)\n",
    "    seas_demo.head(12)\n",
    "else:\n",
    "    'Seasonal model disabled or insufficient data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3282ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5056/1059534697.py:47: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(out[col]):\n",
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5056/1059534697.py:47: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(out[col]):\n",
      "/var/folders/_y/5xqwn5xx5_s9dsm9q1ymghdr0000gn/T/ipykernel_5056/1059534697.py:47: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(out[col]):\n"
     ]
    }
   ],
   "source": [
    "# --- Save outputs ---\n",
    "OUT_DIR = DATA_DIR / 'repurchase_outputs_enhanced'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "excel_engine = None\n",
    "try:\n",
    "    import xlsxwriter  # noqa: F401\n",
    "    excel_engine = 'xlsxwriter'\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openpyxl  # noqa: F401\n",
    "        excel_engine = 'openpyxl'\n",
    "    except ImportError:\n",
    "        excel_engine = None\n",
    "\n",
    "# CSVs\n",
    "repurchase_summary.to_csv(OUT_DIR / 'repurchase_summary.csv', index=False)\n",
    "intervals[['user_id','prev_time','purchase_time','delta_days','serviceType','category']].to_csv(OUT_DIR / 'interpurchase_intervals.csv', index=False)\n",
    "excel_safe(interval_stats).to_csv(OUT_DIR / 'interpurchase_interval_stats.csv', index=False)\n",
    "glob_counts.to_csv(OUT_DIR / 'service_transition_global.csv', index=False)\n",
    "rw_counts.to_csv(OUT_DIR / 'service_transition_recency_weighted.csv', index=False)\n",
    "cat_counts.to_csv(OUT_DIR / 'category_transition_global.csv', index=False)\n",
    "if USE_SEASONAL and not seas_counts.empty:\n",
    "    seas_counts.to_csv(OUT_DIR / 'service_transition_seasonal_monthly.csv', index=False)\n",
    "excel_safe(pred_df).to_csv(OUT_DIR / 'user_next_service_predictions.csv', index=False)\n",
    "excel_safe(last_by_user[['user_id','purchase_time','days_since_last','churn_risk']]).to_csv(OUT_DIR / 'user_churn_flags.csv', index=False)\n",
    "\n",
    "# Excel pack (if an engine exists)\n",
    "if excel_engine:\n",
    "    excel_path = OUT_DIR / 'repurchase_and_prediction_report.xlsx'\n",
    "    with pd.ExcelWriter(excel_path, engine=excel_engine) as writer:\n",
    "        excel_safe(repurchase_summary).to_excel(writer, sheet_name='01_repurchase', index=False)\n",
    "        excel_safe(interval_stats).to_excel(writer, sheet_name='02_interval_stats', index=False)\n",
    "        excel_safe(glob_counts).to_excel(writer, sheet_name='03_transitions_global', index=False)\n",
    "        excel_safe(rw_counts).to_excel(writer, sheet_name='04_transitions_recency', index=False)\n",
    "        excel_safe(cat_counts).to_excel(writer, sheet_name='05_category_transitions', index=False)\n",
    "        if USE_SEASONAL and not seas_counts.empty:\n",
    "            excel_safe(seas_counts).to_excel(writer, sheet_name='06_transitions_seasonal', index=False)\n",
    "        excel_safe(pred_df).to_excel(writer, sheet_name='07_user_predictions', index=False)\n",
    "        excel_safe(last_by_user[['user_id','purchase_time','days_since_last','churn_risk']]).to_excel(writer, sheet_name='08_churn_flags', index=False)\n",
    "    excel_path\n",
    "else:\n",
    "    print(\"No Excel engine available; wrote CSVs only. Install XlsxWriter or openpyxl to get the Excel pack.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedb041",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- **Churn flag** uses `CHURN_THRESHOLD_DAYS` against the dataset's max timestamp. Adjust to fit your business cadence (e.g., 60/90/120 days).\n",
    "- **Recency weighting** uses a **half-life** (`HALFLIFE_DAYS`). Larger values make older transitions count more; smaller values focus the model on recent behavior.\n",
    "- **Personalization** uses shrinkage with `PERSONALIZATION_M`: higher values lean more on global patterns; lower values trust user-specific history more (when available).\n",
    "- **Seasonality** is included for diagnostics. You can condition predictions by current month if you prefer, though recency weighting often performs better with sparse data.\n",
    "- **Category mapping** is derived from `serviceType` using the Walk/Sitting taxonomy; adjust sets in the config if you add types.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
