{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd505554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchase.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28770 entries, 0 to 28769\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   serviceid         28770 non-null  object\n",
      " 1   ownerid           28768 non-null  object\n",
      " 2   ordercreatedtime  28770 non-null  object\n",
      " 3   servicetype       28770 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 899.2+ KB\n",
      "\n",
      "BeforePurchaseDetailsScreen.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36911 entries, 0 to 36910\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   uuid           15209 non-null  object\n",
      " 1   user_id        15209 non-null  object\n",
      " 2   event_time     15209 non-null  object\n",
      " 3   serviceType    15209 non-null  object\n",
      " 4   uuid.1         21702 non-null  object\n",
      " 5   user_id.1      21702 non-null  object\n",
      " 6   event_time.1   21702 non-null  object\n",
      " 7   serviceType.1  21702 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 2.3+ MB\n",
      "\n",
      "DogAdded.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1707 entries, 0 to 1706\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   dogid                 1707 non-null   object \n",
      " 1   ownerid               1707 non-null   object \n",
      " 2   breed                 1707 non-null   object \n",
      " 3   birthday              1707 non-null   object \n",
      " 4   weight                1707 non-null   float64\n",
      " 5   gender                1707 non-null   object \n",
      " 6   isneutered            317 non-null    object \n",
      " 7   havedisabilitiestext  12 non-null     object \n",
      " 8   iseatsfromtheground   317 non-null    object \n",
      " 9   isfightstarter        317 non-null    object \n",
      " 10  istrained             317 non-null    object \n",
      " 11  worryofseparation     164 non-null    object \n",
      " 12  dogaddedtime          1707 non-null   object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 173.5+ KB\n",
      "\n",
      "CatAdded.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 463 entries, 0 to 462\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   catid         463 non-null    object \n",
      " 1   ownerid       463 non-null    object \n",
      " 2   breed         463 non-null    object \n",
      " 3   birthday      463 non-null    object \n",
      " 4   weight        463 non-null    float64\n",
      " 5   gender        463 non-null    object \n",
      " 6   isspayed      463 non-null    bool   \n",
      " 7   cataddedtime  463 non-null    object \n",
      "dtypes: bool(1), float64(1), object(6)\n",
      "memory usage: 25.9+ KB\n",
      "\n",
      "CheckoutPageOpened.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31478 entries, 0 to 31477\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   uuid         31478 non-null  object\n",
      " 1   user_id      31478 non-null  object\n",
      " 2   event_time   31478 non-null  object\n",
      " 3   serviceType  31436 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 983.8+ KB\n",
      "\n",
      "AddressAdded.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1917 entries, 0 to 1916\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   addressid         1917 non-null   object\n",
      " 1   ownerid           1917 non-null   object\n",
      " 2   province          1917 non-null   object\n",
      " 3   district          1917 non-null   object\n",
      " 4   neighborhood      1917 non-null   object\n",
      " 5   addressaddedtime  1917 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 90.0+ KB\n",
      "\n",
      "SignUpCompleted.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10986 entries, 0 to 10985\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          10986 non-null  object\n",
      " 1   signuptime  10986 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 171.8+ KB\n",
      "\n",
      "CreditcardAdded.csv info and head:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 725 entries, 0 to 724\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   creditcardid   725 non-null    object\n",
      " 1   ownerid        725 non-null    object\n",
      " 2   cardaddedtime  725 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder with the CSV files\n",
    "csv_folder = '../tubitakaiagentprojeleriiinverisetleri/'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    purchase_df = pd.read_csv(csv_folder+'Purchase.csv')\n",
    "    before_purchase_df = pd.read_csv(csv_folder+'BeforePurchaseDetailsScreen.csv')\n",
    "    dog_added_df = pd.read_csv(csv_folder+'DogAdded.csv')\n",
    "    cat_added_df = pd.read_csv(csv_folder+'CatAdded.csv')\n",
    "    checkout_opened_df = pd.read_csv(csv_folder+'CheckoutPageOpened.csv')\n",
    "    address_added_df = pd.read_csv(csv_folder+'AddressAdded.csv')\n",
    "    sign_up_df = pd.read_csv(csv_folder+'SignUpCompleted.csv')\n",
    "    creditcard_added_df = pd.read_csv(csv_folder+'CreditcardAdded.csv')\n",
    "\n",
    "    # Inspect each dataframe\n",
    "    print(\"Purchase.csv info and head:\")\n",
    "    purchase_df.info()\n",
    "    purchase_df.head()\n",
    "\n",
    "    print(\"\\nBeforePurchaseDetailsScreen.csv info and head:\")\n",
    "    before_purchase_df.info()\n",
    "    before_purchase_df.head()\n",
    "\n",
    "    print(\"\\nDogAdded.csv info and head:\")\n",
    "    dog_added_df.info()\n",
    "    dog_added_df.head()\n",
    "\n",
    "    print(\"\\nCatAdded.csv info and head:\")\n",
    "    cat_added_df.info()\n",
    "    cat_added_df.head()\n",
    "\n",
    "    print(\"\\nCheckoutPageOpened.csv info and head:\")\n",
    "    checkout_opened_df.info()\n",
    "    checkout_opened_df.head()\n",
    "    \n",
    "    print(\"\\nAddressAdded.csv info and head:\")\n",
    "    address_added_df.info()\n",
    "    address_added_df.head()\n",
    "    \n",
    "    print(\"\\nSignUpCompleted.csv info and head:\")\n",
    "    sign_up_df.info()\n",
    "    sign_up_df.head()\n",
    "    \n",
    "    print(\"\\nCreditcardAdded.csv info and head:\")\n",
    "    creditcard_added_df.info()\n",
    "    creditcard_added_df.head()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}. Please ensure all files are uploaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e3306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caioa\\AppData\\Local\\Temp\\ipykernel_11536\\3220573907.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  conversion_df['purchase_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning and Preparation ---\n",
    "\n",
    "#  Clean BeforePurchaseDetailsScreen.csv\n",
    "# The file has a strange structure with duplicated columns. We need to merge them.\n",
    "before_purchase_part1 = before_purchase_df[['uuid', 'user_id', 'event_time', 'serviceType']].dropna()\n",
    "before_purchase_part2 = before_purchase_df[['uuid.1', 'user_id.1', 'event_time.1', 'serviceType.1']].dropna()\n",
    "before_purchase_part2.columns = ['uuid', 'user_id', 'event_time', 'serviceType']\n",
    "before_purchase_df_cleaned = pd.concat([before_purchase_part1, before_purchase_part2], ignore_index=True)\n",
    "before_purchase_df_cleaned.rename(columns={'user_id': 'ownerid'}, inplace=True)\n",
    "\n",
    "# Corrigir datas para formato misto\n",
    "for col in ['event_time']:\n",
    "    before_purchase_df_cleaned[col] = pd.to_datetime(before_purchase_df_cleaned[col], format='mixed', errors='coerce')\n",
    "\n",
    "#  Clean CheckoutPageOpened.csv\n",
    "checkout_opened_df.rename(columns={'user_id': 'ownerid'}, inplace=True)\n",
    "\n",
    "if 'event_time' in checkout_opened_df.columns:\n",
    "    checkout_opened_df['event_time'] = pd.to_datetime(checkout_opened_df['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "#  Clean Purchase.csv\n",
    "purchase_df.dropna(subset=['ownerid'], inplace=True)\n",
    "purchase_df.rename(columns={'servicetype': 'serviceType'}, inplace=True)\n",
    "\n",
    "if 'event_time' in purchase_df.columns:\n",
    "    purchase_df['event_time'] = pd.to_datetime(purchase_df['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "#  Clean and merge DogAdded.csv and CatAdded.csv\n",
    "dog_added_df['pet_type'] = 'dog'\n",
    "dog_added_df.rename(columns={'dogid': 'petid'}, inplace=True)\n",
    "\n",
    "if 'event_time' in dog_added_df.columns:\n",
    "    dog_added_df['event_time'] = pd.to_datetime(dog_added_df['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "cat_added_df['pet_type'] = 'cat'\n",
    "cat_added_df.rename(columns={'catid': 'petid'}, inplace=True)\n",
    "\n",
    "if 'event_time' in cat_added_df.columns:\n",
    "    cat_added_df['event_time'] = pd.to_datetime(cat_added_df['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "pets_df = pd.concat([dog_added_df, cat_added_df], ignore_index=True)\n",
    "\n",
    "# Owners\n",
    "owners_df = sign_up_df.dropna(subset=['id']).copy()\n",
    "owners_df.rename(columns={'id': 'ownerid'}, inplace=True)\n",
    "owners_df['signuptime'] = pd.to_datetime(owners_df['signuptime'])\n",
    "\n",
    "\n",
    "#  Clean AddressAdded.csv\n",
    "addresses_df = address_added_df.dropna(subset=['ownerid', 'addressid'])\n",
    "\n",
    "if 'event_time' in addresses_df.columns:\n",
    "    addresses_df['event_time'] = pd.to_datetime(addresses_df['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "#  Clean SignUpCompleted.csv\n",
    "sign_up_df_cleaned = sign_up_df.dropna(subset=['id'])\n",
    "sign_up_df_cleaned.rename(columns={'id': 'ownerid'}, inplace=True)\n",
    "\n",
    "if 'event_time' in sign_up_df_cleaned.columns:\n",
    "    sign_up_df_cleaned['event_time'] = pd.to_datetime(sign_up_df_cleaned['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "#  Clean CreditcardAdded.csv\n",
    "creditcard_added_df_cleaned = creditcard_added_df.dropna(subset=['ownerid', 'creditcardid'])\n",
    "\n",
    "if 'event_time' in creditcard_added_df_cleaned.columns:\n",
    "    creditcard_added_df_cleaned['event_time'] = pd.to_datetime(creditcard_added_df_cleaned['event_time'], format='mixed', errors='coerce')\n",
    "\n",
    "# --- Merging DataFrames ---\n",
    "\n",
    "# Merge checkout and purchase data to create a conversion funnel\n",
    "# We are considering a conversion if a user who opened the checkout page for a service type made a purchase for the same service type.\n",
    "# Aggregate checkout and purchase data\n",
    "checkout_agg = checkout_opened_df.groupby(['ownerid', 'serviceType']).size().reset_index(name='checkout_count')\n",
    "purchase_agg = purchase_df.groupby(['ownerid', 'serviceType']).size().reset_index(name='purchase_count')\n",
    "\n",
    "# Merge for conversion analysis\n",
    "conversion_df = pd.merge(checkout_agg, purchase_agg, on=['ownerid', 'serviceType'], how='left')\n",
    "conversion_df['purchase_count'].fillna(0, inplace=True)\n",
    "conversion_df['converted'] = conversion_df['purchase_count'] > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99229b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Conversion Rate: 43.92%\n"
     ]
    }
   ],
   "source": [
    "# --- Analysis and Visualization ---\n",
    "\n",
    "# 1. Overall Conversion Rate\n",
    "total_checkouts = len(checkout_agg)\n",
    "total_conversions = len(conversion_df[conversion_df['converted']])\n",
    "overall_conversion_rate = (total_conversions / total_checkouts) * 100\n",
    "\n",
    "print(f\"Overall Conversion Rate: {overall_conversion_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ca443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 1: CUSTOMER CONVERSION FUNNEL\n",
      "================================================================================\n",
      "This funnel shows where users drop off between signing up and making a purchase.\n",
      "                Step  User Count Overall Conversion from Signup\n",
      "        1. Signed Up       10986                        100.00%\n",
      "      2. Added a Pet        1888                         17.19%\n",
      "    3. Added Address        1771                         16.12%\n",
      "4. Added Credit Card         643                          5.85%\n",
      "  5. Opened Checkout        3213                         29.25%\n",
      "    6. Made Purchase        1645                         14.97%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 1: CUSTOMER CONVERSION FUNNEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"This funnel shows where users drop off between signing up and making a purchase.\")\n",
    "\n",
    "# Get unique owner IDs at each step\n",
    "signed_up_users = set(owners_df['ownerid'])\n",
    "added_pet_users = set(pets_df['ownerid'])\n",
    "added_address_users = set(addresses_df['ownerid'])\n",
    "added_card_users = set(creditcard_added_df['ownerid'])\n",
    "opened_checkout_users = set(checkout_opened_df['ownerid'])\n",
    "purchased_users = set(purchase_df['ownerid'])\n",
    "\n",
    "# Build the funnel data\n",
    "funnel_data = {\n",
    "'Step': [\"1. Signed Up\", \"2. Added a Pet\", \"3. Added Address\", \"4. Added Credit Card\", \"5. Opened Checkout\", \"6. Made Purchase\"],\n",
    "'User Count': [\n",
    "    len(signed_up_users),\n",
    "    len(added_pet_users),\n",
    "    len(added_address_users),\n",
    "    len(added_card_users),\n",
    "    len(opened_checkout_users),\n",
    "    len(purchased_users)\n",
    "]\n",
    "}\n",
    "funnel_df = pd.DataFrame(funnel_data)\n",
    "funnel_df['Overall Conversion from Signup'] = (funnel_df['User Count'] / funnel_df['User Count'].iloc[0] * 100).map('{:.2f}%'.format)\n",
    "\n",
    "print(funnel_df.to_string(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d65a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 2: TOP PERFORMING SEGMENTS\n",
      "================================================================================\n",
      "This analysis identifies the most valuable services, pet types, and locations.\n",
      "\n",
      "--- Top 5 Most Purchased Services ---\n",
      "serviceType\n",
      "Customize      15843\n",
      "AdHoc           7346\n",
      "Planned         4812\n",
      "WalkAndCare      999\n",
      "Boarding         219\n",
      "\n",
      "INSIGHT: 'Customize', 'AdHoc' and 'Planned' are the most popular services.\n",
      "ACTION: Focus marketing campaigns and promotions on these high-demand services.\n",
      "\n",
      "--- Total Purchases by Pet Type ---\n",
      "pet_type\n",
      "dog    3302\n",
      "cat     156\n",
      "\n",
      "INSIGHT: Dog owners make significantly more purchases than cat owners.\n",
      "ACTION: Create marketing campaigns specifically targeting cat owners to boost their engagement.\n",
      "         For dog owners, consider loyalty programs to retain these high-value customers.\n",
      "\n",
      "--- Top 5 Provinces by Number of Purchases ---\n",
      "province\n",
      "İstanbul    4785\n",
      "İzmir        946\n",
      "Ankara       146\n",
      "Mersin        16\n",
      "Muğla          6\n",
      "\n",
      "INSIGHT: Sales are heavily concentrated in İstanbul.\n",
      "ACTION: Double down on marketing efforts in İstanbul. For other cities like İzmir and Ankara,\n",
      "         consider targeted local campaigns to increase market share.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 2: TOP PERFORMING SEGMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"This analysis identifies the most valuable services, pet types, and locations.\")\n",
    "\n",
    "# Merge purchases with pet and address data to get richer context\n",
    "purchases_with_details = pd.merge(purchase_df, pets_df[['ownerid', 'pet_type']].drop_duplicates(), on='ownerid', how='left')\n",
    "purchases_with_details = pd.merge(purchases_with_details, addresses_df[['ownerid', 'province']].drop_duplicates(), on='ownerid', how='left')\n",
    "\n",
    "# Top 5 most purchased services\n",
    "top_services = purchases_with_details['serviceType'].value_counts().nlargest(5)\n",
    "print(\"\\n--- Top 5 Most Purchased Services ---\")\n",
    "print(top_services.to_string())\n",
    "print(\"\\nINSIGHT: 'Customize', 'AdHoc' and 'Planned' are the most popular services.\")\n",
    "print(\"ACTION: Focus marketing campaigns and promotions on these high-demand services.\")\n",
    "\n",
    "# Sales by Pet Type\n",
    "sales_by_pet_type = purchases_with_details.groupby('pet_type')['serviceid'].count().sort_values(ascending=False)\n",
    "print(\"\\n--- Total Purchases by Pet Type ---\")\n",
    "print(sales_by_pet_type.to_string())\n",
    "print(\"\\nINSIGHT: Dog owners make significantly more purchases than cat owners.\")\n",
    "print(\"ACTION: Create marketing campaigns specifically targeting cat owners to boost their engagement.\")\n",
    "print(\"         For dog owners, consider loyalty programs to retain these high-value customers.\")\n",
    "\n",
    "# Top 5 provinces by sales\n",
    "sales_by_province = purchases_with_details['province'].value_counts().nlargest(5)\n",
    "print(\"\\n--- Top 5 Provinces by Number of Purchases ---\")\n",
    "print(sales_by_province.to_string())\n",
    "print(\"\\nINSIGHT: Sales are heavily concentrated in İstanbul.\")\n",
    "print(\"ACTION: Double down on marketing efforts in İstanbul. For other cities like İzmir and Ankara,\")\n",
    "print(\"         consider targeted local campaigns to increase market share.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872324d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 3: TIME FROM SIGNUP TO FIRST PURCHASE\n",
      "================================================================================\n",
      "This analysis shows how long it takes for a new user to become a paying customer.\n",
      "\n",
      "Average time from signup to first purchase: 2 days\n",
      "Median time from signup to first purchase:  0 days\n",
      "\n",
      "INSIGHT: The median time to purchase is a key metric. Half of all new customers convert within this timeframe.\n",
      "ACTION: Consider setting up an automated marketing campaign (e.g., a special offer email)\n",
      "         for users who haven't made a purchase after 1 days to re-engage them.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 3: TIME FROM SIGNUP TO FIRST PURCHASE\")\n",
    "print(\"=\"*80)\n",
    "print(\"This analysis shows how long it takes for a new user to become a paying customer.\")\n",
    "\n",
    "# Merge owner signup time with their purchases\n",
    "time_analysis_df = pd.merge(owners_df, purchase_df, on='ownerid')\n",
    "\n",
    "# Garantir que ambas as colunas são datetime\n",
    "time_analysis_df['ordercreatedtime'] = pd.to_datetime(time_analysis_df['ordercreatedtime'], errors='coerce')\n",
    "time_analysis_df['signuptime'] = pd.to_datetime(time_analysis_df['signuptime'], errors='coerce')\n",
    "\n",
    "# Remover linhas com NaN em 'ordercreatedtime' ou 'ownerid'\n",
    "time_analysis_df = time_analysis_df.dropna(subset=['ordercreatedtime', 'ownerid'])\n",
    "\n",
    "# Find the first purchase time for each user\n",
    "first_purchase_df = time_analysis_df.loc[time_analysis_df.groupby('ownerid')['ordercreatedtime'].idxmin()]\n",
    "\n",
    "# Calculate time to purchase\n",
    "first_purchase_df['time_to_purchase'] = first_purchase_df['ordercreatedtime'] - first_purchase_df['signuptime']\n",
    "\n",
    "# Remove negative deltas which can result from data errors\n",
    "first_purchase_df = first_purchase_df[first_purchase_df['time_to_purchase'] >= pd.Timedelta(0)]\n",
    "\n",
    "avg_time_to_purchase = first_purchase_df['time_to_purchase'].mean()\n",
    "median_time_to_purchase = first_purchase_df['time_to_purchase'].median()\n",
    "\n",
    "print(f\"\\nAverage time from signup to first purchase: {avg_time_to_purchase.days} days\")\n",
    "print(f\"Median time from signup to first purchase:  {median_time_to_purchase.days} days\")\n",
    "print(\"\\nINSIGHT: The median time to purchase is a key metric. Half of all new customers convert within this timeframe.\")\n",
    "print(\"ACTION: Consider setting up an automated marketing campaign (e.g., a special offer email)\")\n",
    "print(f\"         for users who haven't made a purchase after {median_time_to_purchase.days + 1} days to re-engage them.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c281858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS: TOP CONVERTING DOG BREEDS (min. 10 owners)\n",
      "================================================================================\n",
      "                         breed  total_owners  converted_owners conversion_rate\n",
      "                english setter            17                 6          35.29%\n",
      " cavalier king charles spaniel            12                 4          33.33%\n",
      "                     chow chow            19                 6          31.58%\n",
      "            labrador retriever            64                18          28.12%\n",
      "                cocker spaniel            15                 4          26.67%\n",
      "                        beagle            16                 4          25.00%\n",
      "                french bulldog            57                14          24.56%\n",
      "             doberman pinscher            21                 5          23.81%\n",
      "                       mix irk           153                36          23.53%\n",
      "                siberian husky            15                 3          20.00%\n",
      "          jack russell terrier            41                 8          19.51%\n",
      "              golden retriever           217                42          19.35%\n",
      "                    cane corso            16                 3          18.75%\n",
      "                 border collie            18                 3          16.67%\n",
      "             yorkshire terrier            26                 4          15.38%\n",
      "                    rottweiler            20                 3          15.00%\n",
      "                      shih tzu            22                 3          13.64%\n",
      "                     chihuahua            22                 3          13.64%\n",
      "                      maltipoo            74                 9          12.16%\n",
      "       american cocker spaniel            17                 2          11.76%\n",
      "                        poodle           196                23          11.73%\n",
      "amerikan staffordshire terrier            10                 1          10.00%\n",
      "                   japon spitz            10                 1          10.00%\n",
      "      amerikan pitbull terrier            10                 1          10.00%\n",
      "                       maltese           114                11           9.65%\n",
      "                   alman çoban            22                 2           9.09%\n",
      "        english cocker spaniel            12                 1           8.33%\n",
      "                       pointer            12                 1           8.33%\n",
      "                        kangal            12                 1           8.33%\n",
      "              belçika malinois            13                 1           7.69%\n",
      "                     pekingese            17                 1           5.88%\n",
      "                           pug            24                 1           4.17%\n",
      "                    pomeranian            72                 3           4.17%\n",
      "                         breed  total_owners  converted_owners conversion_rate\n",
      "                english setter            17                 6          35.29%\n",
      " cavalier king charles spaniel            12                 4          33.33%\n",
      "                     chow chow            19                 6          31.58%\n",
      "            labrador retriever            64                18          28.12%\n",
      "                cocker spaniel            15                 4          26.67%\n",
      "                        beagle            16                 4          25.00%\n",
      "                french bulldog            57                14          24.56%\n",
      "             doberman pinscher            21                 5          23.81%\n",
      "                       mix irk           153                36          23.53%\n",
      "                siberian husky            15                 3          20.00%\n",
      "          jack russell terrier            41                 8          19.51%\n",
      "              golden retriever           217                42          19.35%\n",
      "                    cane corso            16                 3          18.75%\n",
      "                 border collie            18                 3          16.67%\n",
      "             yorkshire terrier            26                 4          15.38%\n",
      "                    rottweiler            20                 3          15.00%\n",
      "                      shih tzu            22                 3          13.64%\n",
      "                     chihuahua            22                 3          13.64%\n",
      "                      maltipoo            74                 9          12.16%\n",
      "       american cocker spaniel            17                 2          11.76%\n",
      "                        poodle           196                23          11.73%\n",
      "amerikan staffordshire terrier            10                 1          10.00%\n",
      "                   japon spitz            10                 1          10.00%\n",
      "      amerikan pitbull terrier            10                 1          10.00%\n",
      "                       maltese           114                11           9.65%\n",
      "                   alman çoban            22                 2           9.09%\n",
      "        english cocker spaniel            12                 1           8.33%\n",
      "                       pointer            12                 1           8.33%\n",
      "                        kangal            12                 1           8.33%\n",
      "              belçika malinois            13                 1           7.69%\n",
      "                     pekingese            17                 1           5.88%\n",
      "                           pug            24                 1           4.17%\n",
      "                    pomeranian            72                 3           4.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caioa\\AppData\\Local\\Temp\\ipykernel_11536\\2148076246.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dog_owner_breeds['converted'] = dog_owner_breeds['ownerid'].isin(purchasing_users)\n"
     ]
    }
   ],
   "source": [
    "min_owners_per_breed = 10  # Minimum number of owners per breed to consider it significant\n",
    "\n",
    "purchasing_users = set(purchase_df['ownerid'].dropna().unique())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ANALYSIS: TOP CONVERTING DOG BREEDS (min. {min_owners_per_breed} owners)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clean and prepare dog breed data\n",
    "dog_breeds_df = dog_added_df[['ownerid', 'breed']].dropna().copy()\n",
    "dog_breeds_df['breed'] = dog_breeds_df['breed'].str.strip().str.lower()\n",
    "dog_owner_breeds = dog_breeds_df.drop_duplicates()\n",
    "\n",
    "# Flag owners who have converted (made a purchase)\n",
    "dog_owner_breeds['converted'] = dog_owner_breeds['ownerid'].isin(purchasing_users)\n",
    "\n",
    "# Calculate conversion stats for each breed\n",
    "dog_breed_stats = dog_owner_breeds.groupby('breed')['converted'].agg(\n",
    "    total_owners='size',\n",
    "    converted_owners='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Filter for breeds that meet the minimum owner threshold\n",
    "significant_dog_breeds = dog_breed_stats[dog_breed_stats['total_owners'] >= min_owners_per_breed].copy()\n",
    "\n",
    "# Calculate conversion rate and sort\n",
    "significant_dog_breeds['conversion_rate'] = (significant_dog_breeds['converted_owners'] / significant_dog_breeds['total_owners']) * 100\n",
    "top_dog_breeds = significant_dog_breeds.sort_values(by='conversion_rate', ascending=False)\n",
    "top_dog_breeds['conversion_rate'] = top_dog_breeds['conversion_rate'].map('{:.2f}%'.format)\n",
    "\n",
    "# Display the results\n",
    "if top_dog_breeds.empty:\n",
    "    print(f\"\\nNo dog breeds met the minimum owner threshold of {min_owners_per_breed}.\")\n",
    "else:\n",
    "    print(top_dog_breeds.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856394c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS: TOP CONVERTING CAT BREEDS (min. 10 owners)\n",
      "================================================================================\n",
      "                   breed  total_owners  converted_owners conversion_rate\n",
      "            sokak dedisi            12                 4          33.33%\n",
      "           scottish fold            34                 6          17.65%\n",
      "        uzun tüylü tekir            18                 3          16.67%\n",
      "üç renkli tekir (calico)            26                 4          15.38%\n",
      "            sokak kedisi            13                 2          15.38%\n",
      "                   tekir           103                13          12.62%\n",
      "                  sarman            32                 4          12.50%\n",
      "                  smokin            18                 2          11.11%\n",
      "       british shorthair            76                 6           7.89%\n",
      "                  bombay            17                 1           5.88%\n",
      "        british longhair            21                 1           4.76%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INSIGHT: By separating dogs and cats, you can identify the highest-value breeds within each category.\n",
      "ACTION: Tailor your marketing strategies. For example, run a Facebook ad campaign targeting 'Golden Retriever' owners\n",
      "         with ads for 'Boarding' services, and another campaign for 'British Shorthair' owners focusing on 'Grooming'.\n",
      "================================================================================\n",
      "                   breed  total_owners  converted_owners conversion_rate\n",
      "            sokak dedisi            12                 4          33.33%\n",
      "           scottish fold            34                 6          17.65%\n",
      "        uzun tüylü tekir            18                 3          16.67%\n",
      "üç renkli tekir (calico)            26                 4          15.38%\n",
      "            sokak kedisi            13                 2          15.38%\n",
      "                   tekir           103                13          12.62%\n",
      "                  sarman            32                 4          12.50%\n",
      "                  smokin            18                 2          11.11%\n",
      "       british shorthair            76                 6           7.89%\n",
      "                  bombay            17                 1           5.88%\n",
      "        british longhair            21                 1           4.76%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INSIGHT: By separating dogs and cats, you can identify the highest-value breeds within each category.\n",
      "ACTION: Tailor your marketing strategies. For example, run a Facebook ad campaign targeting 'Golden Retriever' owners\n",
      "         with ads for 'Boarding' services, and another campaign for 'British Shorthair' owners focusing on 'Grooming'.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caioa\\AppData\\Local\\Temp\\ipykernel_11536\\858673676.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_owner_breeds['converted'] = cat_owner_breeds['ownerid'].isin(purchasing_users)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ANALYSIS: TOP CONVERTING CAT BREEDS (min. {min_owners_per_breed} owners)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clean and prepare cat breed data\n",
    "cat_breeds_df = cat_added_df[['ownerid', 'breed']].dropna().copy()\n",
    "cat_breeds_df['breed'] = cat_breeds_df['breed'].str.strip().str.lower()\n",
    "cat_owner_breeds = cat_breeds_df.drop_duplicates()\n",
    "\n",
    "# Flag owners who have converted\n",
    "cat_owner_breeds['converted'] = cat_owner_breeds['ownerid'].isin(purchasing_users)\n",
    "\n",
    "# Calculate conversion stats for each breed\n",
    "cat_breed_stats = cat_owner_breeds.groupby('breed')['converted'].agg(\n",
    "    total_owners='size',\n",
    "    converted_owners='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Filter for breeds that meet the minimum owner threshold\n",
    "significant_cat_breeds = cat_breed_stats[cat_breed_stats['total_owners'] >= min_owners_per_breed].copy()\n",
    "\n",
    "# Calculate conversion rate and sort\n",
    "significant_cat_breeds['conversion_rate'] = (significant_cat_breeds['converted_owners'] / significant_cat_breeds['total_owners']) * 100\n",
    "top_cat_breeds = significant_cat_breeds.sort_values(by='conversion_rate', ascending=False)\n",
    "top_cat_breeds['conversion_rate'] = top_cat_breeds['conversion_rate'].map('{:.2f}%'.format)\n",
    "\n",
    "# Display the results\n",
    "if top_cat_breeds.empty:\n",
    "    print(f\"\\nNo cat breeds met the minimum owner threshold of {min_owners_per_breed}.\")\n",
    "else:\n",
    "    print(top_cat_breeds.to_string(index=False))\n",
    "\n",
    "# --- 4. Final Insights ---\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"INSIGHT: By separating dogs and cats, you can identify the highest-value breeds within each category.\")\n",
    "print(\"ACTION: Tailor your marketing strategies. For example, run a Facebook ad campaign targeting 'Golden Retriever' owners\")\n",
    "print(\"         with ads for 'Boarding' services, and another campaign for 'British Shorthair' owners focusing on 'Grooming'.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7996ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176121e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Implementing Technique 1: Logistic Regression to Predict Customer Conversion\n",
      "================================================================================\n",
      "\n",
      "Step 2: Engineering features for the model...\n",
      "Created a dataset with 10986 users and 4 features.\n",
      "\n",
      "Step 3: Training the Logistic Regression model...\n",
      "Model training complete.\n",
      "\n",
      "Step 4: Evaluating the model's performance...\n",
      "\n",
      "Model Accuracy: 99.54%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3239\n",
      "           1       0.96      0.77      0.85        57\n",
      "\n",
      "    accuracy                           1.00      3296\n",
      "   macro avg       0.98      0.89      0.93      3296\n",
      "weighted avg       1.00      1.00      1.00      3296\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3237    2]\n",
      " [  13   44]]\n",
      "(Rows: Actual, Columns: Predicted)\n",
      "\n",
      "Step 5: Interpreting model coefficients...\n",
      "Feature importance (higher coefficient means stronger predictor of purchase):\n",
      "                 Coefficient\n",
      "has_pet             0.945980\n",
      "has_address         0.892878\n",
      "has_credit_card     0.696328\n",
      "tenure_days         0.381286\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Implementing Technique 1: Logistic Regression to Predict Customer Conversion\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "users_df = sign_up_df.rename(columns={'id': 'ownerid'})\n",
    "users_df['signuptime'] = pd.to_datetime(users_df['signuptime'])\n",
    "\n",
    "# --- 1b. Feature Engineering ---\n",
    "print(\"\\nStep 2: Engineering features for the model...\")\n",
    "\n",
    "# Target Variable: Did the user make a purchase?\n",
    "purchasing_users = set(purchase_df['ownerid'].unique())\n",
    "users_df['has_purchased'] = users_df['ownerid'].isin(purchasing_users).astype(int)\n",
    "\n",
    "# Feature 1: Did the user add a pet?\n",
    "pets_df = pd.concat([dog_added_df[['ownerid']], cat_added_df[['ownerid']]])\n",
    "added_pet_users = set(pets_df['ownerid'].unique())\n",
    "users_df['has_pet'] = users_df['ownerid'].isin(added_pet_users).astype(int)\n",
    "\n",
    "# Feature 2: Did the user add an address?\n",
    "added_address_users = set(address_added_df['ownerid'].unique())\n",
    "users_df['has_address'] = users_df['ownerid'].isin(added_address_users).astype(int)\n",
    "\n",
    "# Feature 3: Did the user add a credit card?\n",
    "added_card_users = set(creditcard_added_df['ownerid'].unique())\n",
    "users_df['has_credit_card'] = users_df['ownerid'].isin(added_card_users).astype(int)\n",
    "\n",
    "# Feature 4: User tenure (how long they've been signed up)\n",
    "# We use the latest signup time as 'today' to make the calculation stable\n",
    "latest_signup = users_df['signuptime'].max()\n",
    "users_df['tenure_days'] = (latest_signup - users_df['signuptime']).dt.days\n",
    "\n",
    "# Combine all features into a single model-ready dataframe\n",
    "features = ['has_pet', 'has_address', 'has_credit_card', 'tenure_days']\n",
    "target = 'has_purchased'\n",
    "\n",
    "model_df = users_df[features + [target]].copy()\n",
    "model_df.dropna(inplace=True)\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df[target]\n",
    "\n",
    "print(f\"Created a dataset with {X.shape[0]} users and {X.shape[1]} features.\")\n",
    "\n",
    "# --- 1c. Train the Logistic Regression Model ---\n",
    "print(\"\\nStep 3: Training the Logistic Regression model...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 1d. Evaluate the Model ---\n",
    "print(\"\\nStep 4: Evaluating the model's performance...\")\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"(Rows: Actual, Columns: Predicted)\")\n",
    "\n",
    "# --- 1e. Interpret the Results ---\n",
    "print(\"\\nStep 5: Interpreting model coefficients...\")\n",
    "coefficients = pd.DataFrame(log_reg.coef_[0], X.columns, columns=['Coefficient'])\n",
    "coefficients.sort_values('Coefficient', ascending=False, inplace=True)\n",
    "\n",
    "print(\"Feature importance (higher coefficient means stronger predictor of purchase):\")\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666d411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Implementing Technique 2: K-Means Clustering to Find Customer Segments\n",
      "================================================================================\n",
      "\n",
      "Step 1: Preparing data for clustering (using paying customers only)...\n",
      "\n",
      "Step 2: Engineering features for customer segmentation...\n",
      "Created a dataset of 1645 customers.\n",
      "\n",
      "Step 3: Using the Elbow Method to find the optimal 'K'...\n",
      "Created a dataset of 1645 customers.\n",
      "\n",
      "Step 3: Using the Elbow Method to find the optimal 'K'...\n",
      "\n",
      "Step 4: Running K-Means with K=4 and analyzing segments...\n",
      "\n",
      "--- Customer Segment Profiles ---\n",
      "         recency  frequency  unique_services  customer_count\n",
      "cluster                                                     \n",
      "0          17.82      18.39             2.28             535\n",
      "1          18.12       9.15             1.00             481\n",
      "2          67.92       6.55             1.19             505\n",
      "3          18.84     120.31             2.08              93\n",
      "\n",
      "Step 4: Running K-Means with K=4 and analyzing segments...\n",
      "\n",
      "--- Customer Segment Profiles ---\n",
      "         recency  frequency  unique_services  customer_count\n",
      "cluster                                                     \n",
      "0          17.82      18.39             2.28             535\n",
      "1          18.12       9.15             1.00             481\n",
      "2          67.92       6.55             1.19             505\n",
      "3          18.84     120.31             2.08              93\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"Implementing Technique 2: K-Means Clustering to Find Customer Segments\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 2a. Load and Prepare Data for Clustering ---\n",
    "print(\"\\nStep 1: Preparing data for clustering (using paying customers only)...\")\n",
    "# We only cluster users who have made at least one purchase\n",
    "customers_df = purchase_df.copy()\n",
    "customers_df['ordercreatedtime'] = pd.to_datetime(customers_df['ordercreatedtime'], errors='coerce', utc=True)\n",
    "\n",
    "# --- 2b. Feature Engineering for RFM-like analysis ---\n",
    "print(\"\\nStep 2: Engineering features for customer segmentation...\")\n",
    "latest_purchase_date = customers_df['ordercreatedtime'].max()\n",
    "\n",
    "# Calculate Recency, Frequency, and Tenure (as a proxy for Monetary)\n",
    "segmentation_df = customers_df.groupby('ownerid').agg(\n",
    "    recency=('ordercreatedtime', lambda date: (latest_purchase_date - date.max()).days),\n",
    "    frequency=('serviceid', 'count'),\n",
    "    unique_services=('serviceType', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Created a dataset of {segmentation_df.shape[0]} customers.\")\n",
    "\n",
    "# --- 2c. Find the Optimal Number of Clusters (Elbow Method) ---\n",
    "print(\"\\nStep 3: Using the Elbow Method to find the optimal 'K'...\")\n",
    "features_for_clustering = ['recency', 'frequency', 'unique_services']\n",
    "X_cluster = segmentation_df[features_for_clustering]\n",
    "\n",
    "# Remover linhas com NaN nas features antes de rodar o KMeans\n",
    "X_cluster = X_cluster.dropna()\n",
    "\n",
    "# Scale the features\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "# Calculate inertia for a range of K values\n",
    "inertia = []\n",
    "K_range = range(1, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "optimal_k = 4\n",
    "\n",
    "# --- 2d. Run K-Means and Analyze Segments ---\n",
    "print(f\"\\nStep 4: Running K-Means with K={optimal_k} and analyzing segments...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Adicionar os rótulos de cluster ao DataFrame de segmentação (apenas para os clientes sem NaN)\n",
    "segmentation_df_clean = segmentation_df.loc[X_cluster.index].copy()\n",
    "segmentation_df_clean['cluster'] = cluster_labels\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "cluster_analysis = segmentation_df_clean.groupby('cluster')[features_for_clustering].mean().round(2)\n",
    "cluster_analysis['customer_count'] = segmentation_df_clean['cluster'].value_counts()\n",
    "print(\"\\n--- Customer Segment Profiles ---\")\n",
    "print(cluster_analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd512220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Implementing Technique 3: Market Basket Analysis to Find Service Bundles\n",
      "================================================================================\n",
      "Data successfully one-hot encoded.\n",
      "Created a transaction matrix with 1645 owners and 12 unique services.\n",
      "\n",
      "Step 3: Running the Apriori algorithm to find frequent itemsets...\n",
      "Found 22 frequent itemsets with support >= 1%\n",
      "\n",
      "Top 10 most frequent itemsets (services bought together):\n",
      "     support                     itemsets\n",
      "0   0.540426                      (AdHoc)\n",
      "5   0.475380                    (Planned)\n",
      "11  0.266261             (AdHoc, Planned)\n",
      "3   0.226748                  (Customize)\n",
      "16  0.113070         (Customize, Planned)\n",
      "1   0.085106                   (Boarding)\n",
      "8   0.075380                (WalkAndCare)\n",
      "10  0.066869           (AdHoc, Customize)\n",
      "6   0.054711                    (Sitting)\n",
      "21  0.052888  (AdHoc, Customize, Planned)\n",
      "\n",
      "\n",
      "Step 4: Generating and interpreting association rules...\n",
      "\n",
      "Generated 14 association rules with lift >= 1.\n",
      "\n",
      "--- Top Association Rules ---\n",
      "(Read as: 'If a user buys the services in `antecedents`, they are likely to also buy the ones in `consequents`')\n",
      "            antecedents    consequents   support  confidence      lift\n",
      "4    (AdHoc, Customize)      (Planned)  0.052888    0.790909  1.663741\n",
      "7         (WalkAndCare)     (Boarding)  0.047416    0.629032  7.391129\n",
      "11  (Boarding, Planned)        (AdHoc)  0.010334    0.607143  1.123453\n",
      "1             (Planned)        (AdHoc)  0.266261    0.560102  1.036410\n",
      "6            (Boarding)  (WalkAndCare)  0.047416    0.557143  7.391129\n",
      "10    (AdHoc, Boarding)      (Planned)  0.010334    0.531250  1.117527\n",
      "2           (Customize)      (Planned)  0.113070    0.498660  1.048970\n",
      "0               (AdHoc)      (Planned)  0.266261    0.492688  1.036410\n",
      "9             (Sitting)  (WalkAndCare)  0.024924    0.455556  6.043459\n",
      "8         (WalkAndCare)      (Sitting)  0.024924    0.330645  6.043459\n",
      "\n",
      "\n",
      "--- How to Interpret the Results ---\n",
      "support: The percentage of all customers who purchased this combination of services.\n",
      "confidence: If a customer buys the `antecedent`, what's the probability they also buy the `consequent`?\n",
      "  (e.g., A confidence of 0.6 means 60% of people who bought the first item also bought the second).\n",
      "lift: How much more likely a customer is to buy the `consequent` given they bought the `antecedent`.\n",
      "  (Lift > 1 suggests a strong association).\n",
      "Create Bundles: Look for rules with high confidence and lift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caioa\\anaconda3\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Implementing Technique 3: Market Basket Analysis to Find Service Bundles\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "basket_df = purchase_df[['ownerid', 'serviceType']].dropna().drop_duplicates()\n",
    "\n",
    "\n",
    "basket_encoded = basket_df.pivot_table(index='ownerid', columns='serviceType', aggfunc='size', fill_value=0)\n",
    "\n",
    "# We only care if the service was purchased, not how many times, so we'll convert counts to 1s.\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_encoded = basket_encoded.map(encode_units)\n",
    "print(\"Data successfully one-hot encoded.\")\n",
    "print(f\"Created a transaction matrix with {basket_encoded.shape[0]} owners and {basket_encoded.shape[1]} unique services.\")\n",
    "\n",
    "\n",
    "# --- 3c. Run the Apriori Algorithm ---\n",
    "# This step finds \"frequent itemsets\" - sets of services that are purchased together more\n",
    "# often than a specified threshold (min_support).\n",
    "print(\"\\nStep 3: Running the Apriori algorithm to find frequent itemsets...\")\n",
    "\n",
    "# Set the minimum support threshold. 1% (0.01) is a common starting point.\n",
    "# This means we are looking for itemsets that appear in at least 1% of all transactions.\n",
    "# You can adjust this value: lower it to find more, potentially less significant, itemsets,\n",
    "# or raise it to focus only on the most common combinations.\n",
    "min_support_threshold = 0.01\n",
    "\n",
    "frequent_itemsets = apriori(basket_encoded, min_support=min_support_threshold, use_colnames=True)\n",
    "frequent_itemsets.sort_values('support', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"Found {len(frequent_itemsets)} frequent itemsets with support >= {min_support_threshold:.0%}\")\n",
    "print(\"\\nTop 10 most frequent itemsets (services bought together):\")\n",
    "print(frequent_itemsets.head(10))\n",
    "\n",
    "\n",
    "# --- 3d. Generate and Interpret Association Rules ---\n",
    "# Now we take the frequent itemsets and generate rules like \"If a user buys X, they will also buy Y.\"\n",
    "print(\"\\n\\nStep 4: Generating and interpreting association rules...\")\n",
    "\n",
    "# We generate rules based on 'lift', a metric that measures how much more likely\n",
    "# a user is to buy the `consequents` (e.g., 'Grooming') if they have already bought\n",
    "# the `antecedents` (e.g., 'Boarding'). A lift > 1 is generally considered interesting.\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Sort the rules by confidence and lift to find the strongest relationships.\n",
    "rules.sort_values(['confidence', 'lift'], ascending=[False, False], inplace=True)\n",
    "\n",
    "print(f\"\\nGenerated {len(rules)} association rules with lift >= 1.\")\n",
    "print(\"\\n--- Top Association Rules ---\")\n",
    "print(\"(Read as: 'If a user buys the services in `antecedents`, they are likely to also buy the ones in `consequents`')\")\n",
    "\n",
    "# Display the most relevant columns for easier interpretation\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10).to_string())\n",
    "\n",
    "print(\"\\n\\n--- How to Interpret the Results ---\")\n",
    "print(\"support: The percentage of all customers who purchased this combination of services.\")\n",
    "print(\"confidence: If a customer buys the `antecedent`, what's the probability they also buy the `consequent`?\")\n",
    "print(\"  (e.g., A confidence of 0.6 means 60% of people who bought the first item also bought the second).\")\n",
    "print(\"lift: How much more likely a customer is to buy the `consequent` given they bought the `antecedent`.\")\n",
    "print(\"  (Lift > 1 suggests a strong association).\")\n",
    "\n",
    "print(\"Create Bundles: Look for rules with high confidence and lift.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
